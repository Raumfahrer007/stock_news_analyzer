{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5031315f-bb3b-47b3-a7fe-63160e60baf1",
   "metadata": {},
   "source": [
    "# Exploring the Relationship Between Reddit Sentiment and Stock Price Movements\n",
    "\n",
    "# 1. Introduction & Hypothesis\n",
    "\n",
    "**Research Question**  \n",
    "Is the sentiment in stock-related Reddit posts correlated with short-term stock price movements?\n",
    "\n",
    "This project investigates the relationship between social media sentiment and financial markets by analyzing one year of posts from stock-focused subreddits about three major tech stocks: NVIDIA, Tesla, and Google. The goal is to determine if statistically significant correlations exist between online discussions and stock performance.\n",
    "\n",
    "**Initial Hypothesis**  \n",
    "There is a statistically significant positive correlation between weekly Reddit sentiment scores and weekly stock returns for the selected technology stocks.\n",
    "\n",
    "# 2. Methodology Overview\n",
    "\n",
    "To test this hypothesis, we will:\n",
    "\n",
    "1. **Collect Data**  \n",
    "   Gather historical stock price data for NVDA, TSLA, and GOOGL and collect Reddit posts mentioning these stocks from relevant subreddits over the same time period.\n",
    "\n",
    "2. **Process Data**  \n",
    "   - Clean the text data and address data quality issues\n",
    "   - Calculate sentiment scores for each post using two NLP libraries: TextBlob and VADER\n",
    "   - Calculate weekly stock returns from stock price data\n",
    "   - Combine individual post sentiments into weekly averages\n",
    "   - Merge the weekly sentiment and stock return datasets\n",
    "\n",
    "3. **Analyze & Visualize**  \n",
    "   Use descriptive statistics, visualizations, and correlation analysis to examine the relationship between weekly sentiment and stock returns.\n",
    "\n",
    "4. **Critically Assess**  \n",
    "   Evaluate the strength, statistical significance, and practical importance of any relationships found, while discussing methodological limitations.\n",
    "\n",
    "## 2.1 Data Source Justification\n",
    "\n",
    "**A. Subreddit Selection**  \n",
    "The Reddit data was collected from five stock-focused communities: \"r/wallstreetbets\", \"r/stocks\", \"r/investing\", \"r/StockMarket\", and \"r/StocksAndTrading\".\n",
    "\n",
    "* **Subreddit Justification**  \n",
    "These communities focus specifically on stock discussion and investing, providing direct access to market sentiment. General news or technology forums were avoided because their discussions may not be investment-oriented.\n",
    "\n",
    "* **Excluded Subreddits**  \n",
    "Specialized communities like \"r/Options\" (focused on short-term options trading) and \"r/SecurityAnalysis\" (focused on long-term valuation) were excluded because their specific contexts would introduce different types of signals, making it harder to capture general retail investor sentiment.\n",
    "\n",
    "* **Note on variation**  \n",
    "The selected subreddits represent different investor perspectives, from the speculative nature of \"r/wallstreetbets\" to the long-term focus of \"r/investing\". This variation is acknowledged as part of our dataset.\n",
    "\n",
    "**B. Stock Selection**  \n",
    "The analysis focuses on **NVIDIA (NVDA), Tesla (TSLA), and Alphabet (GOOGL)** for three reasons:\n",
    "\n",
    "1. **High Discussion Volume**  \n",
    "   These are among the most discussed companies on social media, ensuring enough Reddit posts for reliable analysis.\n",
    "\n",
    "2. **Varied Volatility**  \n",
    "   NVDA and TSLA have significant price volatility, while GOOGL is more stable. This mix helps test if sentiment correlations work better with volatile or more stable stocks.\n",
    "\n",
    "3. **Sector Diversity**  \n",
    "   While all are technology companies, they represent different areas (semiconductors, electric vehicles, internet services), allowing us to see if sentiment relationships hold across different business models.\n",
    "\n",
    "**C. Timeframe**  \n",
    "The analysis covers one year of data. This timeframe was chosen because the Reddit API efficiently provides posts from up to one year ago. Stock price data was collected for the same period to ensure alignment. This gives us about one year of data points per stock, which is sufficient for correlation analysis.\n",
    "\n",
    "**Note:** Both stock price data and Reddit posts contain data until **October 28, 2025**\n",
    "\n",
    "## 2.2 Success Criteria\n",
    "\n",
    "This project will be successful if it can:\n",
    "- Identify statistically significant correlation coefficients between weekly sentiment scores and weekly stock returns\n",
    "- Provide a thoughtful discussion about the practical importance of any findings, acknowledging that correlation doesn't mean causation\n",
    "\n",
    "## 2.3 Weekly Analysis Approach\n",
    "\n",
    "After considering daily and weekly options, I chose weekly aggregation because daily sentiment data has too much random noise, while weekly data smooths out these short-term fluctuations, while still captures social media's fast-moving nature. This approach provides enough data points for reliable statistical testing while keeping sentiment and returns properly aligned.\n",
    "\n",
    "**How it works:**\n",
    "- Stock returns: Closing price percentage change from Friday to Friday\n",
    "- Reddit sentiment: Average sentiment from posts from Saturday to Friday\n",
    "- Only complete weeks are included to ensure correct one week stock returns\n",
    "\n",
    "## 2.4 Important Limitations\n",
    "\n",
    "Several limitations should be kept in mind when interpreting the results:\n",
    "\n",
    "* **Correlation vs. Causation**  \n",
    "  This study can find correlations but cannot prove that sentiment causes price moves. Price movements could influence sentiment, or both are driven by other factors like news events.\n",
    "\n",
    "* **Other Influencing Factors**  \n",
    "  Stock prices are affected by many things we're not measuring, like earnings reports, economic data, and world events. Our analysis only looks at sentiment.\n",
    "\n",
    "* **Reddit User Bias**  \n",
    "  The sentiment comes only from Reddit users, who may not represent all investors. Findings apply specifically to this group.\n",
    "\n",
    "## 2.5 Analysis Methods\n",
    "\n",
    "**Descriptive Statistics**  \n",
    "I will use arithmetic means for all summary statistics to maintain consistency when reporting post volumes, sentiment scores, and other metrics.\n",
    "\n",
    "**Correlation Analysis**  \n",
    "Pearson correlation coefficients will measure relationships between sentiment scores and stock returns, which is appropriate for continuous variables and aligns with our hypothesis testing approach.\n",
    "\n",
    "**Sentiment Analysis**\n",
    "Two Natural Language Processing (NLP) libraries are used to calculate sentiment scores:\n",
    "\n",
    "- VADER  \n",
    "  Chosen because it is specifically attuned to sentiments expressed in social media, capturing nuances like slang and emojis.\n",
    "\n",
    "- TextBlob  \n",
    "  Chosen as a well-established, general-purpose sentiment analysis tool to provide a baseline for comparison.\n",
    "\n",
    "Using both libraries allows to assess whether the relationship with stock returns is robust across different methods of measuring sentiment or is specific to one tool's approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bace1d-ddf0-488a-888c-b8b920506fdd",
   "metadata": {},
   "source": [
    "# 3. Packages Installation and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f465d8-a52f-4e4c-9464-f5dcba68f97f",
   "metadata": {},
   "source": [
    "**Note:** The following cell installs the necessary Python packages. This is only required once per runtime environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb9aaa-e252-4f55-80dd-0f282ed337fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install yfinance pandas numpy matplotlib seaborn vaderSentiment textblob python-dotenv statsmodels praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221798f-a4d9-4626-bc67-05ce011314b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, pearsonr, norm\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# APIs & Web\n",
    "import praw\n",
    "import yfinance as yf\n",
    "\n",
    "# NLP & Sentiment\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter Notebook Magic for inline plots\n",
    "%matplotlib inline\n",
    "\n",
    "load_dotenv() # Loads REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT\n",
    "\n",
    "print(\"=== IMPORTS FINISHED ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a76790-d809-4359-b214-3e4aac0ffa51",
   "metadata": {},
   "source": [
    "# 4. Data Collection & Preparation\n",
    "\n",
    "This section implements the data collection pipeline described in Section 2, gathering and preparing both stock market data and Reddit sentiment data for correlation analysis.\n",
    "\n",
    "### 4.0.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b98f18-6061-4b5c-9749-0f9659299cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "data_dir = './data'\n",
    "\n",
    "stocks_config = {\n",
    "    'NVDA': {\n",
    "        'color': 'green',\n",
    "        'keywords': ['nvidia', 'nvda', \"jensen huang\", \"rtx\", \"geforce\"],\n",
    "    },\n",
    "    'TSLA': {\n",
    "        'color': 'orange',\n",
    "        'keywords': ['tesla', 'tsla', 'elon', 'musk'],\n",
    "    },\n",
    "    'GOOGL': {\n",
    "        'color': 'blue',\n",
    "        'keywords': ['google', 'googl', 'alphabet', 'gemini', \"android\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "subreddits_list = ['wallstreetbets', 'stocks', 'investing', 'StockMarket', 'StocksAndTrading']\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "print(\" === CONFIGURATION FINISHED ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9343059-5d22-4966-94a5-08926c2461b4",
   "metadata": {},
   "source": [
    "## 4.1 Stock Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972dd8e1-4508-46f3-bb10-64cc083e9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stocks_data_from_files(tickers=stocks_config.keys()):\n",
    "    \"\"\"\n",
    "    Load stock price data from saved CSV files.\n",
    "    \n",
    "    Args:\n",
    "        tickers (list): List of stock ticker symbols.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Combined DataFrame with all stocks\n",
    "    \"\"\"\n",
    "    stock_data_list = []\n",
    "    \n",
    "    print(\"Loading saved stock data...\")\n",
    "    for ticker in tickers:\n",
    "        file_path = os.path.join(data_dir, f\"stock_data_{ticker}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Stock data file not found: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, parse_dates=['date'], index_col='date')\n",
    "            stock_data_list.append(df)\n",
    "            print(f\"✓ Loaded data for {ticker} (Shape: {df.shape})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Loading stock data for {ticker} failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Combine all stocks into one DataFrame\n",
    "    combined_data = pd.concat(stock_data_list, ignore_index=False)\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "def download_and_save_stocks_data(tickers=stocks_config.keys()):\n",
    "    \"\"\"\n",
    "    Download fresh stock data and save to CSV files.\n",
    "    \n",
    "    Args:\n",
    "        tickers (list): List of stock ticker symbols.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Combined DataFrame with all stocks\n",
    "    \"\"\"\n",
    "    stock_data_list = []\n",
    "    \n",
    "    print(\"Downloading fresh stock data...\")\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365)\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Download data\n",
    "            df = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True, progress=False)\n",
    "            print(df)\n",
    "            if df.empty:\n",
    "                print(f\"✗ No data downloaded for {ticker}\")\n",
    "                continue\n",
    "\n",
    "            # Clean up column structure\n",
    "            df.columns = df.columns.droplevel(1) # Remove stock row from header\n",
    "            df = df.reset_index()\n",
    "            df.columns = ['date', 'close', 'high', 'low', 'open', 'volume']\n",
    "            df = df.set_index('date')\n",
    "            df['ticker'] = ticker\n",
    "            \n",
    "            stock_data_list.append(df)\n",
    "            print(f\"✓ Downloaded data for {ticker} (Shape: {df.shape})\")\n",
    "            \n",
    "            # Save the individual stock data\n",
    "            file_path = os.path.join(data_dir, f\"stock_data_{ticker}.csv\")\n",
    "            df.to_csv(file_path)\n",
    "            print(f\"  Saved to {file_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Downloading stock data for {ticker} failed: {e}\")\n",
    "    \n",
    "    # Combine all stocks into one DataFrame\n",
    "    combined_data = pd.concat(stock_data_list, ignore_index=False)\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "print(\"=== STOCK DATA LOADING ===\")\n",
    "try:\n",
    "    stocks_data_raw = load_stocks_data_from_files()\n",
    "    data_source = \"saved files\"\n",
    "    print(\"=== STOCK DATA LOADED ===\")\n",
    "    print(\"\\n--- Stock Data Info ---\")\n",
    "    print(stocks_data_raw.info())\n",
    "    print(\"\\n--- Stock Data Sample ---\")\n",
    "    print(stocks_data_raw.head(5))\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠ {e}\")\n",
    "    print(\"Note: Download functionality disabled\")\n",
    "    # stocks_data_raw = download_and_save_stocks_data()\n",
    "    # data_source = \"fresh download\"\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3a6db-59e4-49d3-9a21-f1b8e9a73fde",
   "metadata": {},
   "source": [
    "## 4.2 Reddit Data Collection\n",
    "\n",
    "**Reddit Data Sampling Strategy**  \n",
    "Reddit posts were collected using the PRAW library.\n",
    "By setting sort=\"top\" and time_filter=\"year\", I collected the most upvoted posts from the past year that matched the keywords.\n",
    "\n",
    "**Rationale**  \n",
    "This approach prioritizes impactful discourse. Highly upvoted posts represent sentiment seen and endorsed by a larger portion of the subreddit community, providing a stronger signal of collective sentiment than a simple chronological feed, which would be dominated by low-engagement posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c67a3c-136b-41c1-ae41-95095a4df588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reddit_posts_from_file():\n",
    "    \"\"\"\n",
    "    Load Reddit posts from saved CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Reddit posts data\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the Reddit data file is missing\n",
    "    \"\"\"\n",
    "    file_path = f\"{data_dir}/reddit_posts.csv\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Reddit data file not found: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        posts_df = pd.read_csv(file_path, parse_dates=['datetime'])\n",
    "        print(\"✓ Loaded saved Reddit data\")\n",
    "        return posts_df\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Loading Reddit data failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def download_reddit_posts(stocks_config=stocks_config, subreddits=subreddits_list):\n",
    "    \"\"\"\n",
    "    Download fresh Reddit posts from stock-focused subreddits.\n",
    "    \n",
    "    Args:\n",
    "        stocks_config (dict): Contains stock tickers and configurations\n",
    "        subreddits (list): List of stock-focused subreddits\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Downloaded Reddit posts data\n",
    "    \"\"\"\n",
    "    reddit_posts = []\n",
    "    seen_posts = set()  # Tracks post IDs to avoid duplicates\n",
    "\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "            client_secret=os.getenv('REDDIT_CLIENT_SECRET'), \n",
    "            user_agent=os.getenv('REDDIT_USER_AGENT')\n",
    "        )\n",
    "        print(\"✓ Reddit API connected successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Reddit API failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for ticker in stocks_config.keys():\n",
    "        search_query = \" OR \".join(stocks_config[ticker]['keywords'])\n",
    "            \n",
    "        for subreddit_name in subreddits:\n",
    "            print(f\"Collecting posts for {ticker} from r/{subreddit_name}...\")\n",
    "            try:\n",
    "                subreddit = reddit.subreddit(subreddit_name)\n",
    "                subreddit_posts = subreddit.search(\n",
    "                    query=search_query,\n",
    "                    sort=\"top\",\n",
    "                    time_filter=\"year\"\n",
    "                )\n",
    "                \n",
    "                for post in subreddit_posts:\n",
    "                    # Skip duplicates\n",
    "                    if post.id in seen_posts:\n",
    "                        continue\n",
    "                    seen_posts.add(post.id)\n",
    "                    \n",
    "                    content = (post.title + ' ' + (post.selftext or '')).lower()\n",
    "                    \n",
    "                    # Check for stock mentions\n",
    "                    stock_mentions = []\n",
    "                    for stock, config in stocks_config.items():\n",
    "                        keywords = config['keywords']\n",
    "                        if any(keyword in content for keyword in keywords):\n",
    "                            stock_mentions.append(stock)\n",
    "                    \n",
    "                    if stock_mentions:\n",
    "                        post_data = {\n",
    "                            'id': post.id,\n",
    "                            'datetime': datetime.fromtimestamp(post.created_utc),\n",
    "                            'subreddit': subreddit_name,\n",
    "                            'stock': ', '.join(stock_mentions),\n",
    "                            'title': post.title,\n",
    "                            'content': post.selftext,\n",
    "                            'score': post.score,\n",
    "                        }\n",
    "                        reddit_posts.append(post_data)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error with r/{subreddit_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "    posts_df = pd.DataFrame(reddit_posts)\n",
    "    \n",
    "    if not posts_df.empty:\n",
    "        posts_df.to_csv(f\"{data_dir}/reddit_posts.csv\", index=False)\n",
    "        print(f\"✓ Saved {len(posts_df)} Reddit posts to CSV\")\n",
    "    else:\n",
    "        print(\"No Reddit posts collected\")\n",
    "    \n",
    "    return posts_df\n",
    "\n",
    "\n",
    "print(\"=== REDDIT DATA LOADING ===\")\n",
    "try:\n",
    "    reddit_posts_raw = load_reddit_posts_from_file()\n",
    "    data_source = \"saved files\"\n",
    "    print(\"=== REDDIT DATA LOADED ===\")\n",
    "    print(\"\\n--- Reddit Posts Info ---\")\n",
    "    print(reddit_posts_raw.info())\n",
    "    print(\"\\n--- Reddit Posts Sample ---\")\n",
    "    print(reddit_posts_raw.head(5))\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠ {e}\")\n",
    "    print(\"Note: Download functionality disabled\")\n",
    "    # reddit_posts_raw = download_reddit_posts()\n",
    "    # data_source = \"fresh download\"\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073dcf3-14c8-4431-9d8e-583e37ce48c5",
   "metadata": {},
   "source": [
    "## 4.3 Comprehensive Data Inspection & Validation\n",
    "\n",
    "This section provides a foundational understanding of our datasets' structure, quality, and temporal alignment before proceeding with analysis. We examine each data source individually and then assess their compatibility.\n",
    "\n",
    "### 4.3.1 Stock Data Fundamentals\n",
    "\n",
    "We begin by visualizing the price trends and key statistics for the three target stocks. Plotting normalized prices allows for direct comparison of performance and volatility despite different price levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fad76-4b7e-41e7-a5a8-ff4bb45befa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligns close values in horizontal table with index and ticker\n",
    "pivot_data = stocks_data_raw.pivot(columns='ticker', values='close')\n",
    "normalized_pivot = (pivot_data / pivot_data.iloc[0]) * 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for ticker in normalized_pivot.columns:\n",
    "    plt.plot(normalized_pivot.index, normalized_pivot[ticker], \n",
    "             label=ticker, linewidth=2, color=stocks_config[ticker]['color'])\n",
    "\n",
    "plt.title(\"Stock Price Performance (Normalized to Starting Value = 100)\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"Normalized Price (%)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Stocks Fundamentals Summary ---\")\n",
    "for ticker in stocks_config.keys():\n",
    "    stock_data = stocks_data_raw[stocks_data_raw['ticker'] == ticker]\n",
    "    \n",
    "    # Key metrics\n",
    "    price_range = stock_data['close'].max() - stock_data['close'].min()\n",
    "    volatility = stock_data['close'].pct_change().std() * 100\n",
    "    total_return = ((stock_data['close'].iloc[-1] - stock_data['close'].iloc[0]) / stock_data['close'].iloc[0]) * 100\n",
    "    avg_volume = stock_data['volume'].mean()\n",
    "    \n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Time Period: {stock_data.index.min().date()} to {stock_data.index.max().date()}\")\n",
    "    print(f\"  Trading Days: {len(stock_data)}\")\n",
    "    print(f\"  Missing Values: {stock_data.isnull().sum().sum()}\")\n",
    "    print(f\"  Price Range: ${stock_data['close'].min():.2f} - ${stock_data['close'].max():.2f} (${price_range:.2f} spread)\")\n",
    "    print(f\"  Starting Price: ${stock_data['close'].iloc[0]:.2f}\")\n",
    "    print(f\"  Ending Price: ${stock_data['close'].iloc[-1]:.2f}\")\n",
    "    print(f\"  Total Return: {total_return:+.2f}%\")\n",
    "    print(f\"  Daily Volatility: {volatility:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e886c97-55cc-4125-8e4d-298c304d766c",
   "metadata": {},
   "source": [
    "### Initial Observations\n",
    "\n",
    "- **Bullish Market Context**  \n",
    "  All three stocks showed strong positive returns (+43% to +75%) over the analysis period, indicating a generally bullish market environment for technology stocks.\n",
    "\n",
    "- **Volatility Comparison**  \n",
    "  The normalized plot clearly shows the different volatility profiles, with TSLA exhibiting the largest swings (4.27% daily volatility) and GOOGL appearing most stable (2.06% daily volatility), confirming our initial stock selection rationale.\n",
    "\n",
    "- **Performance Divergence**  \n",
    "  The stocks show distinct performance patterns over the year, with significant divergence in returns (TSLA +75% vs NVDA +43%). This variation is ideal for testing our hypothesis, as we have different \"signal\" strengths to analyze.\n",
    "\n",
    "- **Data Quality**  \n",
    "  All three stocks show complete data with no missing values and consistent trading days, providing a solid foundation for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d0133-e2d2-4f16-b30b-6ac2a788e24b",
   "metadata": {},
   "source": [
    "### 4.3.2 Reddit Data Composition & Quality\n",
    "\n",
    "Next, we explore the structure and distribution of the Reddit data. This helps understand the volume, source, and potential quality issues of our sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac345ff0-3908-4dbe-b557-9aa817e44845",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Raw post distribution by stock\n",
    "stock_counts = reddit_posts_raw['stock'].value_counts()\n",
    "stock_counts.plot(kind='bar', ax=axes[0], color='lightblue', edgecolor='black', width=0.8)\n",
    "axes[0].set_title(\"(A) Posts by Stock(s)\", fontweight='bold')\n",
    "axes[0].set_xlabel(\"Stock(s) Mentioned\")\n",
    "axes[0].set_ylabel(\"Number of Posts\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Posts by subreddit\n",
    "subreddit_counts = reddit_posts_raw['subreddit'].value_counts()\n",
    "subreddit_counts.plot(kind='bar', ax=axes[1], color='lightgreen', edgecolor='black', width=0.8)\n",
    "axes[1].set_title(\"(B) Posts by Subreddit\", fontweight='bold')\n",
    "axes[1].set_xlabel(\"Subreddit\")\n",
    "axes[1].set_ylabel(\"Number of Posts\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Posts per month\n",
    "month_counts = reddit_posts_raw['datetime'].dt.to_period('M').value_counts().sort_index()\n",
    "month_counts.plot(kind='bar', ax=axes[2], color='lightcoral', edgecolor='black')\n",
    "axes[2].set_title(\"(C) Posts by Month\", fontweight='bold')\n",
    "axes[2].set_xlabel(\"Month\")\n",
    "axes[2].set_ylabel(\"Number of Posts\")\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text-based statistics (unchanged, just remove the subreddit part)\n",
    "print(\"--- General Statistics ---\")\n",
    "print(f\"   Total Posts Collected: {len(reddit_posts_raw)}\")\n",
    "print(f\"   Date Range: {reddit_posts_raw['datetime'].min().date()} to {reddit_posts_raw['datetime'].max().date()}\")\n",
    "print(f\"   Posts with Empty Content: {reddit_posts_raw['content'].isna().sum()} ({reddit_posts_raw['content'].isna().sum()/len(reddit_posts_raw)*100:.1f}%)\")\n",
    "print(f\"   Posts with Empty Title: {reddit_posts_raw['title'].isna().sum()} ({reddit_posts_raw['title'].isna().sum()/len(reddit_posts_raw)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n--- Posts by Stock Analysis (A) ---\")\n",
    "single_stock_posts = sum(1 for stock in reddit_posts_raw['stock'] if ', ' not in stock)\n",
    "multi_stock_posts = len(reddit_posts_raw) - single_stock_posts\n",
    "print(f\"   Single-stock posts: {single_stock_posts} ({single_stock_posts/len(reddit_posts_raw)*100:.1f}%)\")\n",
    "print(f\"   Multi-stock posts: {multi_stock_posts} ({multi_stock_posts/len(reddit_posts_raw)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n--- Posts by Subreddit Analysis (B) ---\")\n",
    "print(f\"   Avg posts per subreddit: {(len(reddit_posts_raw) / len(subreddit_counts)):.2f}\")\n",
    "print(f\"   Skewness: {skew(subreddit_counts.values):.3f}\")\n",
    "print(f\"   Most active subreddit: r/{subreddit_counts.index[0]} ({subreddit_counts.iloc[0]} posts)\")\n",
    "print(f\"   Least active subreddit: r/{subreddit_counts.index[-1]} ({subreddit_counts.iloc[-1]} posts)\")\n",
    "\n",
    "print(\"\\n--- Posts by Month Analysis (C) ---\")\n",
    "print(f\"   Avg posts per month: {(len(reddit_posts_raw) / len(month_counts)):.2f}\")\n",
    "print(f\"   Skewness: {skew(month_counts.values):.3f}\")\n",
    "print(f\"   Month with most posts: {month_counts.idxmax()} ({month_counts.max()} posts)\")\n",
    "print(f\"   Month with fewest posts: {month_counts.idxmin()} ({month_counts.min()} posts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6daa8-20e0-47ba-bac7-d4efd9fd2a87",
   "metadata": {},
   "source": [
    "### Identified Data Quality Issues\n",
    "\n",
    "1. **Multi-Stock Posts**  \n",
    "   16.3% of posts (163/1003) mention multiple stocks.\n",
    "\n",
    "2. **Subreddit Skew**  \n",
    "   Post distribution across subreddits is moderately skewed, with r/stocks (265 posts) being most active and r/StocksAndTrading (93 posts) least active.\n",
    "\n",
    "3. **Temporal Skew**  \n",
    "   Post volume per month is moderately skewed, with October 2024 having only 5 posts and March 2025 having 122 posts.\n",
    "\n",
    "4. **Content Gaps**  \n",
    "   16.5% of posts lack body content, relying solely on titles for sentiment.\n",
    "\n",
    "*These issues will be addressed in the Data Cleaning section below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b843f13c-62e6-4c68-a49c-f88e73b07c4c",
   "metadata": {},
   "source": [
    "### 4.3.3 Temporal Alignment Validation\n",
    "\n",
    "Finally, we verify that our stock price and Reddit sentiment data cover overlapping time periods, which is essential for a valid correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253bcb7-013b-47b4-b368-5bd596545b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_start = stocks_data_raw.index.min().date()\n",
    "stock_data_end = stocks_data_raw.index.max().date()\n",
    "reddit_posts_start = reddit_posts_raw['datetime'].min().date()\n",
    "reddit_posts_end = reddit_posts_raw['datetime'].max().date()\n",
    "\n",
    "overlap_start = max(stock_data_start, reddit_posts_start)\n",
    "overlap_end = min(stock_data_end, reddit_posts_end)\n",
    "overlap_days = (overlap_end - overlap_start).days + 1\n",
    "\n",
    "print(\"Coverage Summary\")\n",
    "print(f\"Stock Data:      {stock_data_start} to {stock_data_end}\")\n",
    "print(f\"Reddit Data:     {reddit_posts_start} to {reddit_posts_end}\")\n",
    "print(f\"Overlap Period:  {overlap_start} to {overlap_end}\")\n",
    "print(f\"Overlap Duration: {overlap_days} days ({overlap_days/7:.1f} weeks)\")\n",
    "\n",
    "print(\"\\nAlignment Metrics\")\n",
    "total_possible_days = (max(stock_data_end, reddit_posts_end) - min(stock_data_start, reddit_posts_start)).days + 1\n",
    "print(f\"Temporal Coverage: {(overlap_days / total_possible_days) * 100:.2f}% of total period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4dc251-f217-4c1d-b68b-fe47a9b7bff8",
   "metadata": {},
   "source": [
    "### Temporal Alignment Validation Assessment\n",
    "\n",
    "- **Extensive Temporal Overlap**  \n",
    "  365 days of overlapping data (99.73% coverage) provides a solid foundation for weekly analysis.\n",
    "\n",
    "- **Compatible with Weekly Methodology**  \n",
    "  The date ranges contain sufficient data to extract 50-52 complete weeks for our Friday-to-Friday analysis approach (accounting for potential incomplete first/last weeks).\n",
    "\n",
    "- **Conclusion**  \n",
    "  The raw datasets show extensive temporal overlap suitable for weekly aggregation. The exact number of complete Friday-to-Friday weeks will be confirmed during the feature engineering phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77757c3b-55fe-4bd1-9fce-fd2c0e0ae292",
   "metadata": {},
   "source": [
    "# 5. Data Preprocessing & Feature Engineering\n",
    "\n",
    "## 5.1 Addressing Data Quality Issues\n",
    "\n",
    "Based on the initial inspection, the following data quality issues were identified and will be addressed:\n",
    "\n",
    "* **Issue 1: Multi-Stock Posts**  \n",
    "  Finding: 163 posts (16.3%) mention multiple stocks, creating ambiguous sentiment attribution.  \n",
    "  Solution: These posts will be duplicated for each mentioned stock to ensure clean stock-specific analysis.  \n",
    "  Limitation: This approach assumes sentiment applies equally to all mentioned stocks, which may not reflect nuanced discussions where stocks are mentioned with different sentiment valences (e.g., \"I love NVDA but hate TSLA\"). This is a noted constraint of our sentiment aggregation methodology.\n",
    "\n",
    "* **Issue 2: Empty Content**  \n",
    "  Finding: 165 posts (16.5%) have empty content.  \n",
    "  Solution: The post title will be used as the primary content for sentiment analysis, as titles typically contain the core sentiment.\n",
    "\n",
    "* **Issue 3: Temporal Volatility in Post Volume**  \n",
    "  Finding: Post volume shows moderate volatility, with particularly low volume in October 2024 (5 posts).  \n",
    "  Solution: Weekly aggregation will smooth this volatility. The impact of low-volume periods on weekly sentiment averages will be considered during analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80920669-334d-4a79-8b24-2bf9ad18f1ae",
   "metadata": {},
   "source": [
    "## 5.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14513022-3fb2-4847-845c-0963c56d4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy for processing\n",
    "reddit_posts_clean = reddit_posts_raw.copy()\n",
    "print(f\"Initial dataset: {len(reddit_posts_raw)} posts\")\n",
    "\n",
    "# 1. Multi-Stock Posts\n",
    "print(\"\\n1. Processing multi-stock posts...\")\n",
    "\n",
    "expanded_posts = []\n",
    "for i, row in reddit_posts_raw.iterrows():\n",
    "    if ', ' in row['stock']:\n",
    "        # Split stock symbols\n",
    "        multiple_stocks = [s.strip() for s in row['stock'].split(', ')]\n",
    "        for stock in multiple_stocks:\n",
    "            new_row = row.copy()\n",
    "            new_row['stock'] = stock\n",
    "            new_row['was_multi_stock'] = True  # Flag for tracking\n",
    "            expanded_posts.append(new_row)\n",
    "    else:\n",
    "        row['was_multi_stock'] = False  # Flag for tracking\n",
    "        expanded_posts.append(row)\n",
    "\n",
    "reddit_posts_clean = pd.DataFrame(expanded_posts).reset_index(drop=True)\n",
    "print(f\"   After expansion: {len(reddit_posts_clean)} individual stock-mentions\")\n",
    "\n",
    "# 2. Handle Empty Content\n",
    "print(\"\\n2. Handling empty content...\")\n",
    "\n",
    "reddit_posts_clean['content_clean'] = reddit_posts_clean['content'].fillna('')\n",
    "reddit_posts_clean['text_for_analysis'] = reddit_posts_clean['title'] + ' ' + reddit_posts_clean['content_clean']\n",
    "reddit_posts_clean['text_for_analysis'] = reddit_posts_clean['text_for_analysis'].str.strip()\n",
    "\n",
    "# Check for completely empty text\n",
    "empty_text_count = (reddit_posts_clean['text_for_analysis'].str.len() == 0).sum()\n",
    "print(f\"   Posts with no text after cleaning: {empty_text_count}\")\n",
    "\n",
    "print(\"\\n=== DATA CLEANING COMPLETED ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712aa1f-d38a-471f-a99a-0bec3e6400a2",
   "metadata": {},
   "source": [
    "### 5.2.1 Data Cleaning Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148df01-c53d-4fe8-b4d3-c40a3dad46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison figure - 3 rows, 2 columns\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "\n",
    "# Calculate all statistics first\n",
    "stock_counts_before = reddit_posts_raw['stock'].value_counts()\n",
    "stock_counts_after = reddit_posts_clean['stock'].value_counts()\n",
    "subreddit_counts_before = reddit_posts_raw['subreddit'].value_counts()\n",
    "subreddit_counts_after = reddit_posts_clean['subreddit'].value_counts()\n",
    "month_counts_before = reddit_posts_raw['datetime'].dt.to_period('M').value_counts().sort_index()\n",
    "month_counts_after = reddit_posts_clean['datetime'].dt.to_period('M').value_counts().sort_index()\n",
    "\n",
    "# Row 1: Stock Distribution\n",
    "# Left: BEFORE\n",
    "stock_counts_before.plot(kind='bar', ax=axes[0,0], color='lightblue', edgecolor='black', width=0.8)\n",
    "axes[0,0].set_title(\"(A) BEFORE: Posts by Stock\", fontweight='bold')\n",
    "axes[0,0].set_xlabel(\"Stock(s)\")\n",
    "axes[0,0].set_ylabel(\"Number of Posts\")\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Right: AFTER\n",
    "stock_counts_after.plot(kind='bar', ax=axes[0,1], color='lightblue', edgecolor='black', width=0.8)\n",
    "axes[0,1].set_title(\"(B) AFTER: Posts by Stock\", fontweight='bold')\n",
    "axes[0,1].set_xlabel(\"Stock\")\n",
    "axes[0,1].set_ylabel(\"Number of Posts\")\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Row 2: Subreddit Distribution\n",
    "# Left: BEFORE\n",
    "subreddit_counts_before.plot(kind='bar', ax=axes[1,0], color='lightgreen', edgecolor='black', width=0.8)\n",
    "axes[1,0].set_title(\"(C) BEFORE: Posts by Subreddit\", fontweight='bold')\n",
    "axes[1,0].set_xlabel(\"Subreddit\")\n",
    "axes[1,0].set_ylabel(\"Number of Posts\")\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Right: AFTER\n",
    "subreddit_counts_after.plot(kind='bar', ax=axes[1,1], color='lightgreen', edgecolor='black', width=0.8)\n",
    "axes[1,1].set_title(\"(D) AFTER: Posts by Subreddit\", fontweight='bold')\n",
    "axes[1,1].set_xlabel(\"Subreddit\")\n",
    "axes[1,1].set_ylabel(\"Number of Posts\")\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Row 3: Temporal Distribution\n",
    "# Left: BEFORE\n",
    "month_counts_before.plot(kind='bar', ax=axes[2,0], color='lightcoral', edgecolor='black')\n",
    "axes[2,0].set_title(\"(E) BEFORE: Posts by Month\", fontweight='bold')\n",
    "axes[2,0].set_xlabel(\"Month\")\n",
    "axes[2,0].set_ylabel(\"Number of Posts\")\n",
    "axes[2,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Right: AFTER\n",
    "month_counts_after.plot(kind='bar', ax=axes[2,1], color='lightcoral', edgecolor='black')\n",
    "axes[2,1].set_title(\"(F) AFTER: Posts by Month\", fontweight='bold')\n",
    "axes[2,1].set_xlabel(\"Month\")\n",
    "axes[2,1].set_ylabel(\"Number of Posts\")\n",
    "axes[2,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution changes below each plot category\n",
    "print(\"=== DISTRIBUTION CHANGES ===\")\n",
    "\n",
    "print(\"\\n--- Stock Distribution (A/B) ---\")\n",
    "multi_stock_before = len([s for s in reddit_posts_raw['stock'] if ', ' in s])\n",
    "print(f\"   Posts:  {len(reddit_posts_clean)}\")\n",
    "print(f\"     (BEFORE: {len(reddit_posts_raw)} posts, {multi_stock_before} multi-stock posts)\")\n",
    "print(f\"   -> {len(reddit_posts_clean) - len(reddit_posts_raw)} posts increase\")\n",
    "\n",
    "print(\"\\n--- Subreddit Distribution (C/D) ---\")\n",
    "print(f\"   Avg posts per Subreddit: {(len(reddit_posts_clean) / len(subreddit_counts_after)):.2f}\")\n",
    "print(f\"     (BEFORE {(len(reddit_posts_raw) / len(subreddit_counts_before)):.2f})\")\n",
    "print(f\"   Skewness: {skew(subreddit_counts_after.values):.3f}\")\n",
    "print(f\"     (BEFORE: {skew(subreddit_counts_before.values):.3f})\")\n",
    "print(f\"   Most active: r/{subreddit_counts_after.index[0]} ({subreddit_counts_after.iloc[0]} posts)\")\n",
    "print(f\"     (BEFORE: r/{subreddit_counts_before.index[0]} ({subreddit_counts_before.iloc[0]} posts))\")\n",
    "print(f\"   Least active: r/{subreddit_counts_after.index[-1]} ({subreddit_counts_after.iloc[-1]} posts)\")\n",
    "print(f\"     (BEFORE: r/{subreddit_counts_before.index[-1]} ({subreddit_counts_before.iloc[-1]} posts))\")\n",
    "\n",
    "print(\"\\n--- Temporal Distribution (E/F) ---\")\n",
    "print(f\"   Avg posts per Month: {(len(reddit_posts_clean) / len(month_counts_after)):.2f}\")\n",
    "print(f\"     (BEFORE {(len(reddit_posts_raw) / len(month_counts_before)):.2f})\")\n",
    "print(f\"   Skewness: {skew(month_counts_after.values):.3f}\")\n",
    "print(f\"     (BEFORE: {skew(month_counts_before.values):.3f})\")\n",
    "print(f\"   Month with most posts: {month_counts_after.idxmax()} ({month_counts_after.max()} posts)\")\n",
    "print(f\"     (BEFORE: {month_counts_before.idxmax()} ({month_counts_before.max()} posts)\")\n",
    "print(f\"   Month with fewest posts: {month_counts_after.idxmin()} ({month_counts_after.min()} posts)\")\n",
    "print(f\"     (BEFORE: {month_counts_before.idxmin()} ({month_counts_before.min()} posts))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0756afc2-c222-4b2c-8973-5534a6c0e089",
   "metadata": {},
   "source": [
    "### Data Cleaning Validation Assessment\n",
    "\n",
    "- **Multi-Stock Issue Resolved**  \n",
    "  The increase in posts (197 additional posts) confirms successful expansion of multi-stock posts. Each row now contains exactly one stock for clean analysis.\n",
    "\n",
    "- **Data Integrity Maintained**  \n",
    "  The consistent temporal pattern (same peak/valley months) and similar skewness values show that cleaning preserved the underlying data structure while resolving the multi-stock ambiguity.\n",
    "\n",
    "- **Notable Shift in Subreddit Ranking**  \n",
    "  The most active subreddit changed from r/stocks to r/investing after cleaning. This suggests r/investing had more multi-stock discussions, which is reasonable given its broader investment focus compared to r/stocks' single-stock discussions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e8d0a-c7b0-42f5-9eef-310f4889764b",
   "metadata": {},
   "source": [
    "## 5.3 Feature Engineering\n",
    "\n",
    "This section creates the analytical features needed for correlation analysis:\n",
    "\n",
    "**A. Post-Level Feature Engineering**\n",
    "- Add sentiment scores to each post (`textblob_score`, `vader_score`, `combined_score`)\n",
    "- Add categorical `combined_sentiment_label` (positive/neutral/negative) based on the combined score \n",
    "- Add temporal feature `week_ending` (Friday-based) to each post\n",
    "\n",
    "**B. Weekly Aggregation**  \n",
    "- Calculate weekly percentage returns for each stock (Friday-to-Friday)\n",
    "- Drop incomplete weeks (no prior Friday for returns / Friday outside of stock date range)\n",
    "- Aggregate post-level sentiment to weekly means per stock\n",
    "- Align weekly sentiment scores with weekly stock returns using Friday week-ending dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67923878-fc04-4203-9fbe-511786b78089",
   "metadata": {},
   "source": [
    "### 5.3.1 Post-Level Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5776cb-6cad-4e5f-904b-d9925eb26594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzers\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "print(\"✓ Sentiment analyzer initialized\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on text using both VADER and TextBlob analyzers.\n",
    "    \n",
    "    Combines scores from both analyzers with equal weighting and classifies \n",
    "    sentiment as positive, negative, or neutral based on thresholds.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing sentiment scores and labels:\n",
    "            - 'textblob_score' (float): Polarity score from TextBlob [-1.0 to 1.0]\n",
    "            - 'vader_score' (float): Compound score from VADER [-1.0 to 1.0] \n",
    "            - 'combined_score' (float): Average of both scores [-1.0 to 1.0]\n",
    "            - 'combined_sentiment_label' (str): 'positive', 'negative', or 'neutral'\n",
    "    \"\"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # VADER sentiment analysis\n",
    "    try:\n",
    "        vader_compound = vader_analyzer.polarity_scores(text)['compound']\n",
    "    except Exception as e:\n",
    "        print(f\"VADER analysis failed for text: {text[:20]}... Error: {e}\")\n",
    "        vader_compound = 0.0\n",
    "    \n",
    "    # TextBlob sentiment analysis\n",
    "    try:\n",
    "        blob = TextBlob(text)\n",
    "        textblob_polarity = blob.sentiment.polarity\n",
    "    except Exception as e:\n",
    "        print(f\"TextBlob analysis failed for text: {text[:20]}... Error: {e}\")\n",
    "        textblob_polarity = 0.0\n",
    "    \n",
    "    # Combined score (equal weighting)\n",
    "    combined_score = (vader_compound + textblob_polarity) / 2\n",
    "    \n",
    "    # Research-based threshold selection\n",
    "    # Based on common NLP practice: > 0.05 for positive, < -0.05 for negative\n",
    "    # Thresholds provide clear separation while avoiding over-sensitivity to noise\n",
    "    if combined_score > 0.05:\n",
    "        sentiment_label = 'positive'\n",
    "    elif combined_score < -0.05:\n",
    "        sentiment_label = 'negative'\n",
    "    else:\n",
    "        sentiment_label = 'neutral'\n",
    "        \n",
    "    return {\n",
    "        'textblob_score': textblob_polarity,\n",
    "        'vader_score': vader_compound,\n",
    "        'combined_score': combined_score,\n",
    "        'combined_sentiment_label': sentiment_label,\n",
    "    }\n",
    "\n",
    "print(\"\\nApplying sentiment analysis...\")\n",
    "\n",
    "# Applies sentiment analysis and expands results into separate columns\n",
    "sentiment_results = reddit_posts_clean['text_for_analysis'].apply(analyze_sentiment)\n",
    "reddit_posts_sentiment = pd.concat([\n",
    "    reddit_posts_clean, \n",
    "    pd.DataFrame(sentiment_results.tolist())\n",
    "], axis=1)\n",
    "#sentiment_df = pd.json_normalize(sentiment_results)\n",
    "#reddit_posts_sentiment = reddit_posts_clean.assign(**sentiment_df.to_dict('series'))\n",
    "\n",
    "# Validation\n",
    "print(\"✓ Sentiment analysis completed\\n\")\n",
    "print(f\"   Posts with Textblob scores: {len(reddit_posts_sentiment['textblob_score'])}/{len(reddit_posts_clean)}\")\n",
    "print(f\"   Posts with VADER scores: {len(reddit_posts_sentiment['vader_score'])}/{len(reddit_posts_clean)}\")\n",
    "print(f\"   Posts with Combined scores: {len(reddit_posts_sentiment['combined_score'])}/{len(reddit_posts_clean)}\")\n",
    "\n",
    "# Add Temporal Feature for Weekly Analysis\n",
    "print(\"\\nAdding Friday-based week_ending to each post...\")\n",
    "\n",
    "reddit_posts_sentiment['week_ending'] = reddit_posts_sentiment['datetime'].dt.to_period('W-FRI').dt.end_time.dt.date\n",
    "\n",
    "print(f\"   Added week_ending for {reddit_posts_sentiment['week_ending'].nunique()} unique weeks\")\n",
    "print(f\"   Date range: {reddit_posts_sentiment['datetime'].min().date()} to {reddit_posts_sentiment['datetime'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ba787-5caa-49ae-9fef-942359ba2e06",
   "metadata": {},
   "source": [
    "### 5.3.2 Weekly Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033ca06-ceb9-413a-8103-a077e01d15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stock Returns Calculation ---\n",
    "print(\"\\n--- Weekly Stock Returns Calculation ---\")\n",
    "\n",
    "def calculate_weekly_stock_returns(stocks_df):\n",
    "     \"\"\"\n",
    "    Calculate weekly stock returns for correlation analysis.\n",
    "    \n",
    "    Args:\n",
    "        stocks_df (DataFrame): DataFrame with stocks data containing columns ['ticker', 'close'] and DateTimeIndex\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Weekly returns with columns ['ticker', 'week_ending', 'weekly_return']\n",
    "    \"\"\"\n",
    "    weekly_returns_list = []\n",
    "    \n",
    "    for ticker in stocks_df['ticker'].unique():\n",
    "        ticker_data = stocks_df[stocks_df['ticker'] == ticker].copy()\n",
    "        \n",
    "        # Weekly returns (Friday to Friday)\n",
    "        weekly_data = ticker_data.resample('W-FRI').last()\n",
    "        weekly_data['weekly_return'] = weekly_data['close'].pct_change()\n",
    "        weekly_data['week_ending'] = weekly_data.index.date\n",
    "        weekly_data['ticker'] = ticker\n",
    "\n",
    "        # Remove first week if no prior closing (no weekly_return value)\n",
    "        weekly_data = weekly_data.dropna()\n",
    "        \n",
    "        # Remove last week if it is incomplete (last date is not a Friday)\n",
    "        last_week_ending_date = weekly_data.index[-1]\n",
    "        last_actual_date = ticker_data.index[-1]\n",
    "        \n",
    "        if last_week_ending_date > last_actual_date:\n",
    "            weekly_data = weekly_data.iloc[:-1]  # Remove last week\n",
    "        \n",
    "        weekly_returns_list.append(weekly_data)\n",
    "        print(f\"  {ticker}: {len(weekly_data)} complete weeks\")\n",
    "        print(f\"    {weekly_data['week_ending'].min()} - {weekly_data['week_ending'].max()}\")\n",
    "    \n",
    "    # Combine all tickers into one DataFrame\n",
    "    combined_returns = pd.concat(weekly_returns_list, ignore_index=False)\n",
    "    return combined_returns\n",
    "\n",
    "# Calculate returns for all stocks\n",
    "stocks_returns_weekly = calculate_weekly_stock_returns(stocks_data_raw)\n",
    "print(\"✓ Stock returns calculated\")\n",
    "\n",
    "\n",
    "# --- Aggregate Sentiment to Weekly Averages ---\n",
    "print(\"\\n--- Weekly Sentiment Aggregation ---\")\n",
    "def aggregate_weekly_stock_sentiment(sentiment_df, returns_df):\n",
    "    \"\"\"\n",
    "    Aggregate sentiment scores to weekly averages aligned with stock returns.\n",
    "    \n",
    "    Args:\n",
    "        sentiment_df (DataFrame): Reddit posts with sentiment scores and week_endings\n",
    "        returns_df (DataFrame): Weekly stock returns with week_endings\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Weekly sentiment averages with post counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get valid stock-week combinations\n",
    "    valid_combinations = returns_df[['ticker', 'week_ending']].drop_duplicates()\n",
    "    valid_combinations = valid_combinations.rename(columns={'ticker': 'stock'})\n",
    "\n",
    "    # Single merge operation for all stocks\n",
    "    filtered_sentiment = sentiment_df.merge(valid_combinations, on=['stock', 'week_ending'])\n",
    "    \n",
    "    # Single aggregation for all stocks\n",
    "    weekly_sentiment = filtered_sentiment.groupby(['stock', 'week_ending']).agg({\n",
    "        'vader_score': 'mean',\n",
    "        'textblob_score': 'mean', \n",
    "        'combined_score': 'mean',\n",
    "        'id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    weekly_sentiment = weekly_sentiment.rename(columns={'id': 'post_count'})\n",
    "    \n",
    "    # Recalculate sentiment_label based on weekly combined_score mean\n",
    "    weekly_sentiment['combined_sentiment_label'] = weekly_sentiment['combined_score'].apply(\n",
    "        lambda score: 'positive' if score > 0.05 else 'negative' if score < -0.05 else 'neutral'\n",
    "    )\n",
    "\n",
    "    return weekly_sentiment\n",
    "\n",
    "reddit_sentiment_weekly = aggregate_weekly_stock_sentiment(reddit_posts_sentiment, stocks_returns_weekly)\n",
    "print(\"✓ Weekly sentiment aggregation completed\")\n",
    "print(f\"   Total aligned weekly observations: {len(reddit_sentiment_weekly)}\")\n",
    "print(f\"   Unique weeks: {reddit_sentiment_weekly['week_ending'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aceb216-758f-4336-9c64-38f4c064fc4f",
   "metadata": {},
   "source": [
    "## 5.4 Weekly Data Validation\n",
    "\n",
    "Before merging, we validate both weekly datasets using these quality thresholds:\n",
    "\n",
    "**Quality Thresholds:**\n",
    "- **Reliable sentiment**: ≥3 posts per week for stable averages  \n",
    "- **No data gaps**: Continuous weekly observations\n",
    "- **Adequate alignment**: Sufficient overlapping weeks for correlation analysis\n",
    "\n",
    "The validation below assesses these criteria and identifies any reliability concerns.\n",
    "\n",
    "### 5.4.1 Weekly Stock Returns Validation\n",
    "Validate stock returns data for completeness, distribution, and temporal coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e801b-9f76-4cd2-a5ff-59710dec1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "pivot_returns = stock_returns_weekly.pivot(index='week_ending', columns='ticker', values='weekly_return')\n",
    "\n",
    "for ticker in pivot_returns.columns:\n",
    "    plt.plot(pivot_returns.index, pivot_returns[ticker], marker='o', label=ticker, markersize=2, color=stocks_config[ticker]['color'], alpha=0.8)\n",
    "\n",
    "plt.title(\"Weekly Stock Returns\")\n",
    "plt.xlabel(\"Week Ending\")\n",
    "plt.ylabel(\"Weekly Return\")\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for ticker in stock_returns_weekly['ticker'].unique():\n",
    "    ticker_data = stock_returns_weekly[stock_returns_weekly['ticker'] == ticker]\n",
    "    missing_returns = ticker_data['weekly_return'].isna().sum()\n",
    "    print(f\"{ticker}\")\n",
    "    print(f\"  Full Weeks: {len(ticker_data)}\")\n",
    "    print(f\"  Missing Returns: {missing_returns}\")\n",
    "    print(f\"  Date Range: {ticker_data['week_ending'].min()} - {ticker_data['week_ending'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8320e20-8741-45c4-871e-4fafc69dcbb6",
   "metadata": {},
   "source": [
    "### Weekly Stock Returns Validation Assessment:\n",
    "\n",
    "- All three stocks have complete weekly returns data for 51 weeks with no gaps\n",
    "- No missing returns in any stock-week combination\n",
    "\n",
    "### 5.4.2 Weekly Reddit Sentiment Validation\n",
    "\n",
    "Validate Reddit post coverage for gaps, low-volume weeks, and temporal coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254835e2-f016-4b97-ad4c-4420cce5c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# sorted weeks for consistent x-axis\n",
    "all_weeks = sorted(reddit_sentiment_weekly['week_ending'].unique())\n",
    "\n",
    "# plot for each stock\n",
    "for i, stock in enumerate(stocks_config.keys()):\n",
    "    stock_data = reddit_sentiment_weekly[reddit_sentiment_weekly['stock'] == stock].sort_values('week_ending')\n",
    "    \n",
    "    weeks = stock_data['week_ending']\n",
    "    posts = stock_data['post_count']\n",
    "    \n",
    "    bars = axes[i].bar(weeks, posts, color='lightblue', edgecolor='black', width=5)\n",
    "    \n",
    "    # Highlights low-volume weeks (<3 posts) in red\n",
    "    for j, (week, count) in enumerate(zip(weeks, posts)):\n",
    "        if count < 3:\n",
    "            bars[j].set_color('red')\n",
    "    \n",
    "    axes[i].set_title(f\"{stock} - Weekly Post Volume\", fontweight='bold')\n",
    "    axes[i].set_ylabel(\"Number of Posts\")\n",
    "    axes[i].axhline(y=3, color='red', linestyle='--', alpha=0.7, label=\"Minimum threshold (3 posts)\")\n",
    "    axes[i].legend()\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Only show x-axis labels on bottom plot to reduce clutter\n",
    "    if i < 2:\n",
    "        axes[i].set_xticklabels([])\n",
    "    else:\n",
    "        axes[i].set_xlabel(\"Week Ending Date\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add summary statistics\n",
    "print(\"\\n--- Weekly Post Volume Summary ---\")\n",
    "for stock in stocks_config.keys():\n",
    "    stock_data = reddit_sentiment_weekly[reddit_sentiment_weekly['stock'] == stock]\n",
    "    low_volume = len(stock_data[stock_data['post_count'] < 3])\n",
    "    zero_posts = len(stock_data[stock_data['post_count'] == 0])\n",
    "    \n",
    "    print(f\"{stock}:\")\n",
    "    print(f\"  Total weeks: {len(stock_data)}\")\n",
    "    print(f\"  Total number of posts: {stock_data['post_count'].sum()}\")\n",
    "    print(f\"  Weeks with <3 posts: {low_volume} ({low_volume/len(stock_data)*100:.2f}%)\")\n",
    "    print(f\"  Weeks with 0 posts: {zero_posts}\")\n",
    "    print(f\"  Average posts/week: {stock_data['post_count'].mean():.1f}\")\n",
    "\n",
    "print(\"\\nGeneral:\")\n",
    "print(f\"  Total weeks: {len(reddit_sentiment_weekly)}\")\n",
    "print(f\"  Total number of posts: {reddit_sentiment_weekly['post_count'].sum()}\")\n",
    "low_volume_all = len(reddit_sentiment_weekly[reddit_sentiment_weekly['post_count'] < 3])\n",
    "zero_posts_all = len(reddit_sentiment_weekly[reddit_sentiment_weekly['post_count'] == 0])\n",
    "print(f\"  Weeks with <3 posts: {low_volume_all} ({low_volume_all/len(reddit_sentiment_weekly)*100:.2f}%)\")\n",
    "print(f\"  Weeks with 0 posts: {zero_posts_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce2b81-88e1-499d-b3fd-84e8b5cee027",
   "metadata": {},
   "source": [
    "### Weekly Reddit Sentiment Validation Assessment\n",
    "\n",
    "**Reliability**  \n",
    "- **NVDA**: Excellent coverage with only 3.92% of weeks having low post volume\n",
    "- **TSLA**: Good coverage though with higher low-volume weeks (9.80%) - sentiment may be noisier\n",
    "- **GOOGL**: Solid coverage with 7.84% low-volume weeks\n",
    "\n",
    "**Quality**  \n",
    "- 92.81% of weeks (142/153) have adequate post volume (≥3 posts)\n",
    "- Consistent 51-week coverage across all stocks\n",
    "- No gaps in temporal coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf7944-ff17-42b2-95a5-f1605d23c962",
   "metadata": {},
   "source": [
    "### 5.4.3 Weekly Alignment Validation\n",
    "\n",
    "Verify that the weekly aggregated sentiment data and stock returns are correctly aligned by date and stock identifier to ensure valid correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09539b04-4c28-4a8f-b97a-fac87242222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in stocks_config.keys():\n",
    "    # Returns data for this stock\n",
    "    returns_data = stock_returns_weekly[stock_returns_weekly['ticker'] == ticker]\n",
    "    returns_weeks = set(returns_data['week_ending'])\n",
    "    \n",
    "    # Sentiment data for this stock\n",
    "    sentiment_data = reddit_sentiment_weekly[reddit_sentiment_weekly['stock'] == ticker]\n",
    "    sentiment_weeks = set(sentiment_data['week_ending'])\n",
    "    \n",
    "    # Calculate alignment\n",
    "    overlapping_weeks = returns_weeks & sentiment_weeks\n",
    "    total_possible_weeks = len(returns_weeks | sentiment_weeks)\n",
    "    alignment_percentage = (len(overlapping_weeks) / total_possible_weeks) * 100\n",
    "    \n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Weekly Returns:    {len(returns_weeks)} weeks, {returns_data['week_ending'].min()} to {returns_data['week_ending'].max()}\")\n",
    "    print(f\"  Weekly Sentiment:  {len(sentiment_weeks)} weeks, {sentiment_data['week_ending'].min()} to {sentiment_data['week_ending'].max()}\")\n",
    "    print(f\"  Overlap:           {len(overlapping_weeks)} weeks ({alignment_percentage:.1f}% alignment)\")\n",
    "    \n",
    "    # Show any mismatches\n",
    "    if len(overlapping_weeks) < len(returns_weeks):\n",
    "        missing_in_sentiment = returns_weeks - sentiment_weeks\n",
    "        missing_in_returns = sentiment_weeks - returns_weeks\n",
    "        if missing_in_sentiment:\n",
    "            print(f\"    Missing in sentiment: {len(missing_in_sentiment)} weeks\")\n",
    "        if missing_in_returns:\n",
    "            print(f\"    Missing in returns: {len(missing_in_returns)} weeks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1fdeaf-8fd0-482e-8c5b-2f8393339c37",
   "metadata": {},
   "source": [
    "### Weekly Alignment Validation Assessment\n",
    "\n",
    "**Temporal Coverage**  \n",
    "- All three stocks have weekly returns for 51 weeks with no gaps.\n",
    "   - (2024-11-08 - 2025-10-24)\n",
    "- All three stocks have sentiment data for 51 weeks with no gaps.\n",
    "   - (2024-11-08 - 2025-10-24)\n",
    "    \n",
    "**Alignment Quality**  \n",
    "With 92.81% of weeks having adequate post volume (≥3 posts) and consistent 51-week coverage, the dataset provides a robust foundation for correlation analysis.\n",
    "\n",
    "**Methodological Decision**  \n",
    "All 51 weeks will be included in the correlation analysis. While weeks with fewer than 3 posts may have noisier sentiment measurements, they represent genuine periods of lower market discussion and provide a complete picture of the relationship over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed1d7c-d6f9-4f9b-a627-5ae3a430a561",
   "metadata": {},
   "source": [
    "## 5.5 Dataset Merging & Final Validation\n",
    "\n",
    "This section combines the engineered weekly features into the final analysis dataset:\n",
    "\n",
    "- **Merge** weekly sentiment scores with weekly stock returns\n",
    "- **Validate** the merged dataset structure and completeness\n",
    "\n",
    "### 5.5.1 Dataset Merging\n",
    "\n",
    "With both datasets validated and temporally aligned, we now merge the weekly sentiment averages with weekly stock returns to create the final analysis dataset. The merge uses an inner join on `stock` and `week_ending` columns to ensure only complete stock-week pairs are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576fecb8-12da-463a-b51c-3399e8a00df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare returns data for merging\n",
    "returns_for_merge = stocks_returns_weekly[['ticker', 'week_ending', 'weekly_return']].copy()\n",
    "returns_for_merge = returns_for_merge.rename(columns={'ticker': 'stock'})\n",
    "\n",
    "print(f\"Returns data for merging: {len(returns_for_merge)} observations\")\n",
    "print(f\"Sentiment data for merging: {len(reddit_sentiment_weekly)} observations\")\n",
    "\n",
    "# Merge the datasets\n",
    "merged_data = reddit_sentiment_weekly.merge(\n",
    "    returns_for_merge,\n",
    "    on=['stock', 'week_ending'],\n",
    "    how='inner'  # Should be perfect match due to previous alignment\n",
    ")\n",
    "\n",
    "print(f\"✓ Merged dataset created: {len(merged_data)} observations\")\n",
    "print(f\"  Stocks: {merged_data['stock'].unique().tolist()}\")\n",
    "print(f\"  Weeks: {merged_data['week_ending'].nunique()}\")\n",
    "print(f\"  Date range: {merged_data['week_ending'].min()} to {merged_data['week_ending'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a404b87-4580-41fe-9820-e0fa3d4d531d",
   "metadata": {},
   "source": [
    "### 5.5.2 Merged Dataset Validation\n",
    "\n",
    "After merging, we perform a final validation to ensure the dataset is analysis-ready.  \n",
    "This checks data integrity, completeness, and confirms the merged dataset meets all quality standards for correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3cb134-65b6-4655-8d4d-8d6907e9b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 1. Dataset Structure ---\")\n",
    "print(f\"   Rows: {len(merged_data)}\")\n",
    "print(f\"   Columns: {len(merged_data.columns)}\")\n",
    "print(f\"   Shape: {merged_data.shape}\")\n",
    "\n",
    "print(\"\\n--- 2. Data Completeness ---\")\n",
    "print(f\"   Missing values:\")\n",
    "for col in merged_data.columns:\n",
    "    missing = merged_data[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"     ✗ {col}: {missing} missing\")\n",
    "    else:\n",
    "        print(f\"     ✓ {col}: complete\")\n",
    "\n",
    "print(\"\\n--- 3. Data Quality ---\")\n",
    "print(f\"   Expected observations: {merged_data['stock'].nunique()} stocks × {merged_data['week_ending'].nunique()} weeks = {merged_data['stock'].nunique() * merged_data['week_ending'].nunique()}\")\n",
    "print(f\"     Actual observations: {len(merged_data)}\")\n",
    "duplicates = merged_data.duplicated(subset=['stock', 'week_ending']).sum()\n",
    "print(f\"   Duplicate stock-week pairs: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442cddd4-5c5d-4d4c-a4c6-f5366c94dd34",
   "metadata": {},
   "source": [
    "### Merged Dataset Validation Assessment\n",
    "\n",
    "- Perfect structural alignment: 153 rows matching the expected 3 stocks × 51 weeks\n",
    "- No duplicate stock-week pairs or missing values across all 8 columns\n",
    "- Complete temporal coverage with all weeks represented for each stock\n",
    "\n",
    "The merged dataset successfully integrates all engineered features and meets all quality standards for correlation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9007662-1af0-41ec-8081-20e319e57657",
   "metadata": {},
   "source": [
    "# 6. Exploratory Data Analysis\n",
    "\n",
    "Before testing our hypothesis about sentiment and returns, we first examine the fundamental characteristics of our key variables. This initial exploration ensures we understand what we're working with and helps identify any data issues that might affect our correlation analysis.\n",
    "\n",
    "## 6.1 Distribution of Target Variable: Weekly Returns\n",
    "\n",
    "We begin by analyzing the weekly returns for NVDA, TSLA, and GOOGL — the core metric we're trying to relate to Reddit sentiment. This examination answers basic but crucial questions: How volatile are these stocks? What are their typical weekly movements? Are there extreme values that could distort our results?\n",
    "\n",
    "The findings here establish the baseline behavior of our dependent variable and inform whether standard correlation methods are appropriate for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff575fb5-626d-496d-b6d0-a94faeff9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots for histograms and boxplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for i, ticker in enumerate(stocks_config.keys()):\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker]\n",
    "    returns = stock_data['weekly_return']\n",
    "    \n",
    "    # Histogram (top row)\n",
    "    axes[i].hist(returns, bins=15, color=stocks_config[ticker]['color'], alpha=0.6, edgecolor='black')\n",
    "    axes[i].axvline(returns.mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {returns.mean():.3f}\")\n",
    "    axes[i].set_title(f\"{ticker} - Weekly Returns Distribution\")\n",
    "    axes[i].set_xlabel(\"Weekly Return\")\n",
    "    axes[i].set_ylabel(\"Frequency\")\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Distribution Summary ---\")\n",
    "for ticker in stocks_config.keys():\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker]\n",
    "    returns = stock_data['weekly_return']\n",
    "    \n",
    "    print(f\"\\n{ticker}:\")\n",
    "    print(f\"  Mean: {returns.mean() * 100:.2f}%\")\n",
    "    print(f\"  Std Dev: {returns.std() * 100:.2f}%\")\n",
    "    print(f\"  Min: {returns.min() * 100:.2f}%\")\n",
    "    print(f\"  Max: {returns.max() * 100:.2f}%\")\n",
    "    print(f\"  Skewness: {skew(returns):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7445a-d708-44b3-891e-08aac21a2ecb",
   "metadata": {},
   "source": [
    "### Insights from Weekly Returns Distribution\n",
    "\n",
    "The descriptive statistics reveal distinct risk-return profiles across the three technology stocks during the analysis period:\n",
    "\n",
    "**Volatility Hierarchy**\n",
    "- **TSLA** exhibits the highest volatility (Std Dev: 8.21%), confirming its reputation as the most unpredictable of the three\n",
    "- **NVDA** shows moderate volatility (Std Dev: 6.44%) \n",
    "- **GOOGL** demonstrates the most stable behavior (Std Dev: 4.42%), approximately half as volatile as TSLA\n",
    "\n",
    "**Return Characteristics**\n",
    "- All three stocks achieved positive average weekly returns, consistent with the overall bullish market context\n",
    "- TSLA delivered the highest average returns (1.41%) but with substantially greater risk\n",
    "- GOOGL offered the most favorable risk-return profile with steady, positive performance\n",
    "\n",
    "**Distribution Patterns:**\n",
    "- **TSLA** shows significant positive skewness (0.8109), showing a tendency for occasional large positive returns that pull the average upward\n",
    "- **GOOGL** displays a nearly symmetric distribution (skewness: 0.0014), while **NVDA** shows very slight negative skewness (-0.0149), showing more balanced upside and downside movements\n",
    "- The maximum weekly gains exceed maximum losses for all stocks, particularly for TSLA which saw a 29.01% weekly surge\n",
    "\n",
    "#### Implications for Sentiment Analysis\n",
    "The varying volatility levels provide an ideal testing ground for our hypothesis.\n",
    "If Reddit sentiment correlates with returns, we would expect to see the strongest relationship with TSLA (highest volatility) and the weakest with GOOGL (lowest volatility)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888aabd-fcbb-4c39-a4f7-7a7956c99a13",
   "metadata": {},
   "source": [
    "## 6.2 Distribution of Predictor Variables: Sentiment Scores\n",
    "\n",
    "We now examine the sentiment scores that form the basis of our predictive analysis. Understanding the distribution of VADER, TextBlob, and combined sentiment scores is crucial for assessing whether these variables contain meaningful signal for our correlation analysis.\n",
    "\n",
    "This exploration answers fundamental questions: Do we have sufficient variation in sentiment scores to detect relationships with returns? Are the scores systematically biased in one direction? How consistent are the different sentiment measurement methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361074e-8951-45c7-a4de-fe16d4fd0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "\n",
    "sentiment_columns = ['vader_score', 'textblob_score', 'combined_score']\n",
    "sentiment_names = ['VADER Sentiment', 'TextBlob Sentiment', 'Combined Sentiment']\n",
    "\n",
    "for i, ticker in enumerate(stocks_config.keys()):\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker]\n",
    "    \n",
    "    for j, (col, name) in enumerate(zip(sentiment_columns, sentiment_names)):\n",
    "        sentiment_data = stock_data[col]\n",
    "        \n",
    "        # Histograms\n",
    "        axes[j, i].hist(sentiment_data, bins=15, color=stocks_config[ticker]['color'], alpha=0.6, edgecolor='black')\n",
    "        axes[j, i].axvline(sentiment_data.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                          label=f\"Mean: {sentiment_data.mean():.3f}\")\n",
    "        axes[j, i].axvline(0, color='black', linestyle='-', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        if i == 0:\n",
    "            axes[j, i].set_ylabel(f\"{name}\\nFrequency\")\n",
    "        if j == 0:\n",
    "            axes[j, i].set_title(f\"{ticker}\")\n",
    "        if j == 2:\n",
    "            axes[j, i].set_xlabel(\"Sentiment Score\")\n",
    "            \n",
    "        axes[j, i].legend()\n",
    "        axes[j, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print sentiment statistics\n",
    "print(\"\\n--- Sentiment Summary ---\")\n",
    "for ticker in stocks_config.keys():\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker]\n",
    "    \n",
    "    print(f\"\\n{ticker}:\")\n",
    "    for col, name in zip(sentiment_columns, sentiment_names):\n",
    "        sentiment_data = stock_data[col]\n",
    "        mean_str = f\"Mean: {sentiment_data.mean():.3f}\"\n",
    "        std_dev_str = f\"Std Dev: {sentiment_data.std():.3f}\"\n",
    "        min_str = f\"Min: {sentiment_data.min():.3f}\"\n",
    "        max_str = f\"Max: {sentiment_data.max():.3f}\"\n",
    "        skew_str = f\"Skewness: {skew(sentiment_data):.3f}\"\n",
    "        pos_str = f\">0: {(sentiment_data > 0).sum()}/51 weeks ({(sentiment_data > 0).sum()/51*100:.2f}%)\"\n",
    "        print(f\"  --{name}--\")\n",
    "        print(f\"    {mean_str:12} | {std_dev_str:12} | {min_str:12} | {max_str:12} | {skew_str:18}\")\n",
    "        print(f\"    | {pos_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2acf6b2-73d7-43b1-98a1-da5a7f90cfa4",
   "metadata": {},
   "source": [
    "### Insights from Sentiment Scores Distribution\n",
    "\n",
    "The sentiment analysis reveals striking patterns in Reddit discussion sentiment across the three technology stocks:\n",
    "\n",
    "**Overwhelming Optimism Bias**\n",
    "- **GOOGL** generated the most consistently positive sentiment, with 94.12-98.04% of weeks showing positive scores across all measurement methods\n",
    "- **NVDA** follows with 84.31-92.16% positive weeks, demonstrating strong but slightly more varied sentiment\n",
    "- **TSLA** showed the most balanced sentiment profile, though still predominantly positive (76.47-82.35% of weeks)\n",
    "\n",
    "**Methodological Divergence**\n",
    "- **VADER** captured much stronger sentiment signals (means: 0.24-0.52) with greater variability, reflecting its sensitivity to social media language patterns\n",
    "- **TextBlob** produced more conservative, tightly clustered scores (means: 0.06-0.10), suggesting it may be less responsive to the extremes of financial discussion\n",
    "- The **combined scores** effectively averaged these approaches, providing moderate sentiment values with reduced volatility\n",
    "\n",
    "**Sentiment Strength Hierarchy**\n",
    "- **GOOGL** attracted the most favorable sentiment overall, with combined scores averaging 0.31\n",
    "- **NVDA** followed closely with 0.24 average combined sentiment\n",
    "- **TSLA** generated the most tempered positive sentiment at 0.15, potentially reflecting its controversial nature among investors\n",
    "\n",
    "**Distribution Characteristics**\n",
    "- All three stocks show negative skewness in VADER and combined scores, indicating occasional strongly negative sentiment weeks punctuate generally positive discussion\n",
    "- TextBlob distributions are generally more symmetric, though GOOGL shows notable positive skewness (0.810)\n",
    "- The substantial standard deviations confirm meaningful week-to-week sentiment variation exists for correlation analysis\n",
    "\n",
    "#### Critical Implication for Hypothesis Testing\n",
    "The strong positive sentiment bias across all stocks creates a challenging environment for detecting correlation with returns. With sentiment predominantly positive, we are essentially testing whether *degrees of optimism* rather than sentiment direction correlates with price movements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91988c-3366-46b6-a6b0-d7661ef0f094",
   "metadata": {},
   "source": [
    "## 6.3 Temporal Patterns Analysis\n",
    "\n",
    "We now examine how post volume, sentiment, and returns evolve over time to identify potential trends, seasonal patterns, or structural breaks that could influence our correlation analysis. Understanding the temporal dynamics is crucial for recognizing whether external events or systematic patterns might confound the relationship between Reddit sentiment and stock returns.\n",
    "\n",
    "This analysis helps answer critical questions: Are there periods of unusually high market discussion that coincide with major price movements? Do sentiment and returns show coordinated shifts over time that might suggest broader market regimes? Are there specific events or time periods that disproportionately drive any observed relationships?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13126a-b7bd-43a7-b897-208282e67137",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "for i, ticker in enumerate(stocks_config.keys()):\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker].sort_values('week_ending')\n",
    "    bar_color = stocks_config[ticker]['color']\n",
    "    \n",
    "    # Plot 1: Post Volume over Time\n",
    "    axes[0, i].bar(stock_data['week_ending'], stock_data['post_count'], \n",
    "                   color=bar_color, alpha=0.6, width=5)\n",
    "    axes[0, i].set_title(f\"{ticker} - Weekly Post Volume\")\n",
    "    axes[0, i].set_ylabel(\"Number of Posts\")\n",
    "    axes[0, i].tick_params(axis='x', rotation=45)\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Sentiment over Time\n",
    "    axes[1, i].plot(stock_data['week_ending'], stock_data['combined_score'], \n",
    "                    color=bar_color, alpha=0.6, linewidth=2.5, marker='o', markersize=3, label=\"Combined Sentiment\")\n",
    "    axes[1, i].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1, i].set_title(f\"{ticker} - Weekly Sentiment Trend\")\n",
    "    axes[1, i].set_ylabel(\"Sentiment Score\")\n",
    "    axes[1, i].tick_params(axis='x', rotation=45)\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "    axes[1, i].legend()\n",
    "    \n",
    "    # Plot 3: Returns over Time\n",
    "    axes[2, i].plot(stock_data['week_ending'], stock_data['weekly_return'], \n",
    "                    color=bar_color, alpha=0.6, linewidth=2.5, marker='o', markersize=3, label=\"Weekly Returns\")\n",
    "    axes[2, i].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[2, i].set_title(f\"{ticker} - Weekly Returns Trend\")\n",
    "    axes[2, i].set_ylabel(\"Weekly Return\")\n",
    "    axes[2, i].set_xlabel(\"Week Ending\")\n",
    "    axes[2, i].tick_params(axis='x', rotation=45)\n",
    "    axes[2, i].grid(True, alpha=0.3)\n",
    "    axes[2, i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate temporal statistics\n",
    "print(\"\\n--- Temporal Summary ---\")\n",
    "for ticker in stocks_config.keys():\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker].sort_values('week_ending')\n",
    "    \n",
    "    print(f\"\\n{ticker}:\")\n",
    "    \n",
    "    # Post volume trends\n",
    "    early_posts, mid_posts, late_posts = np.array_split(stock_data['post_count'], 3)\n",
    "    print(f\"  Post Volume Trend: Early {early_posts.mean():.2f} → Mid {mid_posts.mean():.2f} → Late {late_posts.mean():.2f}\")\n",
    "    \n",
    "    # Sentiment trends\n",
    "    early_sentiment, mid_sentiment, late_sentiment = np.array_split(stock_data['combined_score'], 3)\n",
    "    print(f\"  Sentiment Trend: Early {early_sentiment.mean():.3f} → Mid {mid_sentiment.mean():.3f} → Late {late_sentiment.mean():.3f}\")\n",
    "    \n",
    "    # Volatility clusters\n",
    "    high_vol_periods = len(stock_data[abs(stock_data['weekly_return']) > stock_data['weekly_return'].std()])\n",
    "    print(f\"  High Volatility Weeks: {high_vol_periods}/51 ({high_vol_periods/51*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2581d-55e0-46e8-ab02-476a72b3b989",
   "metadata": {},
   "source": [
    "### Insights from Temporal Patterns Analysis\n",
    "\n",
    "The temporal analysis reveals distinct patterns in how discussion volume, sentiment, and returns evolved throughout the study period:\n",
    "\n",
    "**Discussion Volume Dynamics**\n",
    "- **TSLA** experienced the most dramatic volume fluctuations, with a mid-period surge to 12.9 posts/week followed by a sharp decline, suggesting episodic bursts of intense market discussion\n",
    "- **NVDA** and **GOOGL** showed more stable discussion patterns, while both see increased attention in the latter period\n",
    "\n",
    "**Sentiment Trajectories**\n",
    "- All three stocks exhibited a **mid-period sentiment dip**, with TSLA experiencing the most pronounced decline to near-neutral levels (0.072)\n",
    "- **GOOGL** maintained the highest sentiment throughout, though it still followed the broader pattern of mid-period cooling\n",
    "- The parallel sentiment declines across all stocks suggest **market-wide factors** rather than company-specific news drove sentiment patterns\n",
    "\n",
    "**Volatility and Discussion Patterns**\n",
    "- **GOOGL** showed the highest proportion of high-volatility weeks (33.3%), challenging its perception as the most stable stock\n",
    "- The timing of TSLA's volume surge aligns with its mid-period sentiment decline, potentially indicating contentious or uncertain discussion phases\n",
    "- NVDA's late-period volume increase coincides with sentiment recovery, suggesting renewed optimistic discussion\n",
    "\n",
    "#### Critical Implications for Correlation Analysis\n",
    "The synchronized mid-period sentiment decline across all stocks represents a potential confounding factor. If this sentiment dip coincided with broader market conditions that also affected returns, it could create a spurious correlation. The episodic nature of discussion volume, particularly for TSLA, means sentiment measurements may be less reliable during low-engagement periods.\n",
    "\n",
    "The temporal patterns suggest we should test whether the sentiment-return relationship remains consistent across different phases of our study period, particularly comparing the sentiment-dip period against the more optimistic early and late phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a37fa0-1ba4-4695-9ede-eb1852f93f72",
   "metadata": {},
   "source": [
    "# 7. Relationship Analysis\n",
    "\n",
    "We now directly test our core hypothesis by examining the quantitative relationship between Reddit sentiment and stock returns. This section moves from exploratory analysis to hypothesis testing, using statistical methods to measure whether the anticipated connection between online discussion sentiment and price movements actually exists in our data.\n",
    "\n",
    "The correlation analysis provides the foundational evidence for answering our research question: Is there a statistically significant relationship between weekly sentiment scores and weekly stock returns across our three technology stocks?\n",
    "\n",
    "## 7.1 Correlation Analysis (Quantitative Measurement)\n",
    "\n",
    "This analysis quantifies the linear relationship between Reddit sentiment scores and weekly stock returns using Pearson correlation coefficients. For each stock and sentiment measurement method, we calculate the correlation strength (r), statistical significance (p-value), and 95% confidence intervals to assess both the magnitude and precision of any observed relationships.\n",
    "\n",
    "The R-squared values provide crucial context by indicating what percentage of return variation is explained by sentiment scores, while confidence intervals help determine whether observed correlations are distinguishable from random chance. Together, these metrics allow us to evaluate both the statistical and practical significance of any relationships found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4531cb-e972-4e99-bd87-caaac0d1d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER\n",
    "sentiment_names = {\n",
    "    'Vader': 'vader_score',\n",
    "    'TextBlob': 'textblob_score',\n",
    "    'Combined': 'combined_score'\n",
    "}\n",
    "\n",
    "def correlation_ci(r, n, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval for Pearson correlation coefficient using Fisher z-transform\"\"\"\n",
    "    # Fisher z-transform\n",
    "    z = np.arctanh(r)\n",
    "    \n",
    "    # Standard error\n",
    "    se = 1 / np.sqrt(n - 3)\n",
    "    \n",
    "    # Z critical value\n",
    "    z_crit = norm.ppf(1 - (1 - confidence) / 2)\n",
    "    \n",
    "    # Confidence interval in z-space\n",
    "    z_lower = z - z_crit * se\n",
    "    z_upper = z + z_crit * se\n",
    "    \n",
    "    # Transform back to r\n",
    "    r_lower = np.tanh(z_lower)\n",
    "    r_upper = np.tanh(z_upper)\n",
    "    \n",
    "    return (r_lower, r_upper)\n",
    "\n",
    "# Calculate comprehensive correlation matrix\n",
    "correlation_results = []\n",
    "\n",
    "for ticker in stocks_config.keys():\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker]\n",
    "    n_weeks = len(stock_data)\n",
    "    \n",
    "    for sentiment_name, sentiment_col in sentiment_names.items():\n",
    "        corr_coef, p_value = pearsonr(stock_data[sentiment_col], stock_data['weekly_return'])\n",
    "        \n",
    "        # Calculate confidence interval\n",
    "        ci_lower, ci_upper = correlation_ci(corr_coef, n_weeks)\n",
    "        \n",
    "        correlation_results.append({\n",
    "            'stock': ticker,\n",
    "            'sentiment_method': sentiment_name,\n",
    "            'correlation': corr_coef,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'r_squared': corr_coef ** 2,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'ci_width': ci_upper - ci_lower\n",
    "        })\n",
    "\n",
    "# Results dataframe\n",
    "corr_df = pd.DataFrame(correlation_results)\n",
    "\n",
    "print(\"\\n--- Correlation Results ---\")\n",
    "for ticker in stocks_config.keys():\n",
    "    print(f\"\\n{ticker}:\")\n",
    "    stock_corr = corr_df[corr_df['stock'] == ticker]\n",
    "    for _, row in stock_corr.iterrows():\n",
    "        significance = \"***\" if row['p_value'] < 0.05 else \"\"\n",
    "        print(f\"  {row['sentiment_method']:12} r = {row['correlation']:6.3f}{significance}\")\n",
    "        print(f\"              95% CI: [{row['ci_lower']:6.3f}, {row['ci_upper']:6.3f}]\")\n",
    "        print(f\"              p = {row['p_value']:5.3f}, R² = {row['r_squared']:5.3f}\")\n",
    "\n",
    "print(\"\\n--- Correlation Heatmap ---\")\n",
    "pivot_corr = corr_df.pivot(index=\"sentiment_method\", columns='stock', values='correlation')\n",
    "pivot_pvals = corr_df.pivot(index=\"sentiment_method\", columns='stock', values='p_value')\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(pivot_corr, annot=True, cmap='RdYlBu', center=0, \n",
    "            vmin=-0.5, vmax=0.5, fmt='.3f', \n",
    "            cbar_kws={'label': \"Correlation Coefficient\"})\n",
    "plt.title(\"Correlation: Sentiment Scores vs Weekly Returns\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Correlation Summary ---\")\n",
    "print(f\"Total significant correlations: {corr_df['significant'].sum()}/9 ({corr_df['significant'].sum()/9*100:.1f}%)\")\n",
    "print(f\"Average correlation coefficient: {corr_df['correlation'].mean():.3f}\")\n",
    "print(f\"Average R²: {corr_df['r_squared'].mean():.3f}\")\n",
    "print(f\"Average 95% CI width: {corr_df['ci_width'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n--- Method Comparison ---\")\n",
    "method_summary = corr_df.groupby('sentiment_method').agg({\n",
    "    'correlation': 'mean',\n",
    "    'significant': 'sum',\n",
    "    'r_squared': 'mean',\n",
    "    'ci_width': 'mean'\n",
    "}).round(3)\n",
    "print(method_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbfdd2b-3c04-4ae2-a3a4-53f14bf89c49",
   "metadata": {},
   "source": [
    "### Insights from Correlation Analysis\n",
    "\n",
    "The correlation results reveal a clear and definitive pattern: **no statistically significant relationship exists** between Reddit sentiment scores and weekly stock returns for any of the three technology stocks during our study period.\n",
    "\n",
    "#### Key Findings\n",
    "\n",
    "- **Statistical Significance**\n",
    "   - **0 out of 9 correlations** reached statistical significance (p < 0.05)\n",
    "   - All p-values range from 0.386 to 0.889, far above the conventional significance threshold\n",
    "   - **All 95% confidence intervals include zero**, confirming the correlations are indistinguishable from random chance\n",
    "\n",
    "- **Correlation Strength and Precision**\n",
    "   - Correlation coefficients are exceptionally weak, ranging from -0.077 to +0.124\n",
    "   - The average correlation across all stocks and methods is only 0.052\n",
    "   - **Wide confidence intervals** (average width: 0.548) indicate substantial uncertainty in all estimates\n",
    "   - Even the most extreme plausible values (CI bounds) show only modest potential relationships\n",
    "\n",
    "- **Explanatory Power**\n",
    "   - R-squared values are negligible, ranging from 0.000 to 0.015\n",
    "   - On average, sentiment scores explain only **0.6%** of the variation in weekly returns\n",
    "   - Even the strongest relationship (TSLA VADER) explains just 1.5% of return variation\n",
    "\n",
    "- **Method Consistency**\n",
    "   - All three sentiment measurement methods show similarly weak results and uncertainty\n",
    "   - No method demonstrates clear superiority in detecting relationships with returns\n",
    "   - Identical confidence interval widths (0.548-0.549) across methods confirm consistent precision\n",
    "\n",
    "#### Critical Implications\n",
    "\n",
    "**These results directly reject our initial hypothesis.** We hypothesized that \"there is a statistically significant positive correlation between weekly Reddit sentiment scores and weekly stock returns,\" but the evidence shows:\n",
    "\n",
    "1. **No statistical significance** - All correlations fail significance tests and confidence intervals include zero\n",
    "2. **Substantial uncertainty** - Wide confidence intervals show we cannot precisely estimate any true relationship\n",
    "3. **Minimal explanatory power** - Even if relationships exist, they explain virtually none of the return variation\n",
    "4. **Methodological consistency** - The null finding holds across all sentiment measurement approaches\n",
    "\n",
    "While the average correlation (0.052) is technically positive as hypothesized, the combination of statistical insignificance, wide confidence intervals, and trivial explanatory power means we cannot claim support for the proposed relationship. The findings strongly suggest that weekly aggregated Reddit sentiment, as measured in this study, does not correlate with short-term stock price movements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123654a-c3d7-405d-9f94-e368539aee4a",
   "metadata": {},
   "source": [
    "## 7.2 Visual Relationship Analysis (Qualitative Assessment)\n",
    "\n",
    "While correlation coefficients provide numerical evidence, visual analysis allows us to qualitatively assess the relationship patterns. This section complements our statistical findings by examining scatterplots with regression lines and confidence intervals, helping us understand whether the weak correlation results reflect genuinely scattered data or are influenced by outliers or non-linear patterns.\n",
    "\n",
    "The visual assessment answers a crucial question: Do the scatterplots confirm the absence of relationship suggested by our correlation analysis, or do they reveal patterns that might be masked by the linear correlation assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0870925d-239f-4386-b5cc-7d33269975c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "\n",
    "for i, ticker in enumerate(stocks_config.keys()):\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker]\n",
    "    \n",
    "    for j, (sentiment_name, sentiment_col) in enumerate(sentiment_names.items()):\n",
    "        # Scatterplot with regression line and confidence interval\n",
    "        sns.regplot(\n",
    "            x=sentiment_col, \n",
    "            y=\"weekly_return\", \n",
    "            data=stock_data,\n",
    "            ax=axes[i, j],\n",
    "            scatter_kws={'alpha': 0.6, 'color': stocks_config[ticker]['color'], 's': 50},\n",
    "            line_kws={'color': 'red', 'linewidth': 2},\n",
    "            ci=95  # 95% confidence interval\n",
    "        )\n",
    "        \n",
    "        corr = stock_data[sentiment_col].corr(stock_data['weekly_return'])\n",
    "        \n",
    "        axes[i, j].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "        axes[i, j].axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "        axes[i, j].set_xlabel(f\"{sentiment_name}\")\n",
    "        if j == 0:\n",
    "            axes[i, j].set_ylabel(\"Weekly Return\")\n",
    "        axes[i, j].set_title(f\"{ticker}\\nr = {corr:.3f}\")\n",
    "        axes[i, j].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Visual Pattern Analysis ---\")\n",
    "for i, ticker in enumerate(stocks_config.keys()):\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker]\n",
    "    \n",
    "    print(f\"\\n{ticker}\")\n",
    "    \n",
    "    # Check for outlier influence\n",
    "    for sentiment_name, sentiment_col in sentiment_names.items():\n",
    "        sentiment_data = stock_data[sentiment_col]\n",
    "        returns_data = stock_data['weekly_return']\n",
    "        \n",
    "        # Extreme points\n",
    "        extreme_sentiment = (abs(sentiment_data - sentiment_data.mean()) > 2 * sentiment_data.std())\n",
    "        extreme_returns = (abs(returns_data - returns_data.mean()) > 2 * returns_data.std())\n",
    "        extreme_points = sum(extreme_sentiment & extreme_returns)\n",
    "        \n",
    "        print(f\"  {sentiment_name:12}: {extreme_points} extreme points in both dimensions\")\n",
    "    \n",
    "    # Check data distribution across quadrants\n",
    "    combined_data = stock_data['combined_score']\n",
    "    returns_data = stock_data['weekly_return']\n",
    "    \n",
    "    q1 = sum((combined_data > 0) & (returns_data > 0))  # Positive sentiment, positive returns\n",
    "    q2 = sum((combined_data < 0) & (returns_data > 0))  # Negative sentiment, positive returns  \n",
    "    q3 = sum((combined_data < 0) & (returns_data < 0))  # Negative sentiment, negative returns\n",
    "    q4 = sum((combined_data > 0) & (returns_data < 0))  # Positive sentiment, negative returns\n",
    "    \n",
    "    print(f\"  Quadrant Distribution: ++:{q1}/51, -+:{q2}/51, --:{q3}/51, +-:{q4}/51\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d113c-ca49-49bf-adfd-1a1c7b773dad",
   "metadata": {},
   "source": [
    "### Insights from Visual Relationship Analysis\n",
    "\n",
    "The scatterplots and pattern analysis provide compelling visual confirmation of the weak correlation results, with consistent patterns across all three sentiment measurement methods.\n",
    "\n",
    "#### Data Distribution Patterns (Based on Combined Sentiment Scores)\n",
    "\n",
    "- **GOOGL** shows the most concentrated clustering:\n",
    "   - **32 of 51 weeks** (63%) fall in the \"positive sentiment, positive returns\" quadrant\n",
    "   - Only **1 week** shows the counter-intuitive \"negative sentiment, positive returns\" pattern\n",
    "   - **Zero weeks** with aligned negative sentiment and negative returns\n",
    "   - This creates a dense cluster in the upper-right quadrant with minimal spread\n",
    "\n",
    "- **NVDA** displays moderate dispersion:\n",
    "   - **29 of 51 weeks** (57%) in the positive-positive quadrant  \n",
    "   - Balanced distribution across other quadrants, though still sparse\n",
    "   - Shows slightly more variation than GOOGL but without clear directional pattern\n",
    "\n",
    "- **TSLA** exhibits the most scattered distribution:\n",
    "   - Only **18 of 51 weeks** (35%) in the positive-positive quadrant\n",
    "   - Substantial presence in all quadrants, including **22 weeks** of \"positive sentiment, negative returns\"\n",
    "   - This widespread scattering explains TSLA's slightly higher (but still non-significant) correlation coefficients\n",
    "\n",
    "**Critical Consistency Across Methods**\n",
    "- **No extreme outliers** were found in any of the three sentiment methods (VADER, TextBlob, Combined)\n",
    "- The visual patterns in scatterplots are remarkably consistent across all sentiment measurement approaches\n",
    "- This methodological consistency strengthens our confidence that the absence of relationship is genuine, not an artifact of one particular sentiment analysis technique\n",
    "\n",
    "#### Overall Conclusion\n",
    "The visual evidence strongly supports the correlation results across all sentiment measurement methods. The scatterplots show genuinely scattered data without clear linear trends, confirming that no meaningful relationship exists between weekly sentiment scores and returns in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280c657-7b63-4c0f-9470-eb7b647d15fc",
   "metadata": {},
   "source": [
    "## 7.3 Alignment Analysis (Categorical Relationship)\n",
    "\n",
    "This analysis provides a simplified, categorical perspective on the sentiment-return relationship by measuring how often sentiment direction aligns with return direction. Instead of measuring correlation strength, we calculate the simple percentage of weeks where sentiment and returns point in the same direction.\n",
    "\n",
    "This approach answers a more fundamental question: If we used Reddit sentiment as a simple directional indicator, how often would it correctly match the actual movement of stock prices?\n",
    "\n",
    "**Methodological Note**  \n",
    "We compare alignment rates against expected random chance, which accounts for the baseline probability of alignment given the individual distributions of positive sentiment and positive returns. The expected random alignment is calculated as: \n",
    "`P(Positive Sentiment) × P(Positive Returns) + P(Negative Sentiment) × P(Negative Returns)`. This ensures we don't misinterpret high alignment rates that simply reflect both series being predominantly positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed9cde-1028-4bbd-b441-f3d29fe0da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_results = []\n",
    "\n",
    "for ticker in stocks_config.keys():\n",
    "    stock_data = merged_data[merged_data['stock'] == ticker]\n",
    "    \n",
    "    for sentiment_name, sentiment_col in sentiment_names.items():\n",
    "        # Sentiment direction (positive/negative)\n",
    "        sentiment_positive = stock_data[sentiment_col] > 0\n",
    "        returns_positive = stock_data['weekly_return'] > 0\n",
    "        \n",
    "        # Calculate alignment\n",
    "        aligned_weeks = (sentiment_positive & returns_positive) | (~sentiment_positive & ~returns_positive)\n",
    "        alignment_percentage = aligned_weeks.mean() * 100\n",
    "        \n",
    "        # Calculate expected random alignment\n",
    "        p_positive_sentiment = sentiment_positive.mean()\n",
    "        p_positive_returns = returns_positive.mean()\n",
    "        expected_random = (p_positive_sentiment * p_positive_returns + \n",
    "                          (1 - p_positive_sentiment) * (1 - p_positive_returns)) * 100\n",
    "        \n",
    "        alignment_results.append({\n",
    "            'stock': ticker,\n",
    "            'method': sentiment_name,\n",
    "            'alignment_percentage': alignment_percentage,\n",
    "            'expected_random': expected_random,\n",
    "            'above_random': alignment_percentage > expected_random,\n",
    "            'weeks_aligned': aligned_weeks.sum(),\n",
    "            'total_weeks': len(aligned_weeks)\n",
    "        })\n",
    "\n",
    "alignment_df = pd.DataFrame(alignment_results)\n",
    "\n",
    "print(\"\\n--- Alignment Results ---\")\n",
    "for ticker in stocks_config.keys():\n",
    "    print(f\"\\n{ticker}:\")\n",
    "    stock_alignment = alignment_df[alignment_df['stock'] == ticker]\n",
    "    \n",
    "    for _, row in stock_alignment.iterrows():\n",
    "        diff_from_random = row['alignment_percentage'] - row['expected_random']\n",
    "        diff_symbol = \"+\" if diff_from_random > 0 else \"\"\n",
    "        print(f\"  {row['method']:12}: {row['alignment_percentage']:5.2f}% aligned \"\n",
    "              f\"({row['weeks_aligned']}/{row['total_weeks']} weeks)\")\n",
    "        print(f\"               Expected by random chance: {row['expected_random']:5.2f}% \"\n",
    "              f\"({diff_symbol}{diff_from_random:4.2f}%)\")\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\n--- Alignment Summary ---\")\n",
    "overall_alignment = alignment_df['alignment_percentage'].mean()\n",
    "overall_expected = alignment_df['expected_random'].mean()\n",
    "\n",
    "print(f\"Overall Alignment: {overall_alignment:.2f}%\")\n",
    "print(f\"Overall Expected by Chance: {overall_expected:.2f}%\")\n",
    "print(f\"Average Above Random: {overall_alignment - overall_expected:+.2f}%\")\n",
    "print(f\"Methods Above Random: {alignment_df['above_random'].sum()}/9 \"\n",
    "      f\"({alignment_df['above_random'].sum()/9*100:.1f}%)\")\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x_pos = np.arange(len(alignment_df))\n",
    "\n",
    "colors = [stocks_config[ticker]['color'] for ticker in alignment_df['stock']]\n",
    "\n",
    "bars = ax.bar(x_pos, alignment_df['alignment_percentage'], color=colors, alpha=0.6, width=0.8, label=\"Actual Alignment\")\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(y=50, color='red', linestyle='--', linewidth=2, label=\"Pure Random (50%)\")\n",
    "\n",
    "# Expected random lines\n",
    "for i, (_, row) in enumerate(alignment_df.iterrows()):\n",
    "    ax.plot([i-0.4, i+0.4], [row['expected_random'], row['expected_random']], \n",
    "            'black', linewidth=2, alpha=0.8, label=\"Expected Random\" if i == 0 else \"\")\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xlabel(\"Stock & Method\")\n",
    "ax.set_ylabel(\"Alignment Percentage (%)\")\n",
    "ax.set_title(\"Sentiment-Return Directional Alignment vs Random Chance\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f\"{row['stock']}\\n{row['method']}\" for _, row in alignment_df.iterrows()], rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df8e2b-49e9-4b18-a77c-c167947e70fe",
   "metadata": {},
   "source": [
    "### Insights from Alignment Analysis\n",
    "\n",
    "The alignment analysis **reinforces the consistent pattern** observed throughout our analysis: Reddit sentiment provides no meaningful directional signal for stock returns, with alignment rates that match or even underperform random chance.\n",
    "\n",
    "#### Key Findings\n",
    "\n",
    "**Overall Performance**\n",
    "- **Overall alignment (55.1%)** actually **underperforms** the expected random chance (56.9%) by -1.7%\n",
    "- Only **3 out of 9 method-stock combinations** beat random expectations\n",
    "- The best-performing combination (GOOGL TextBlob) only exceeds random chance by 2.5%\n",
    "  \n",
    "**Stock-Specific Patterns**\n",
    "- **GOOGL** shows the strongest but still minimal performance:\n",
    "   - Alignment rates range from 62.7% to 66.7%\n",
    "   - However, these rates are almost identical to what random chance would predict (63.0%-64.1%)\n",
    "   - The apparent \"success\" is largely due to GOOGL's consistent positive returns and sentiment\n",
    "\n",
    "- **NVDA** demonstrates near-random performance:\n",
    "   - Alignment rates hover around 57-61%, barely differing from expected random levels\n",
    "   - The methods show inconsistent performance, with TextBlob and Combined Sentiment slightly above and Vader below chance\n",
    "\n",
    "- **TSLA** shows concerning underperformance:\n",
    "   - All three sentiment methods **underperform** random chance by 3.2-10.8%\n",
    "   - TextBlob sentiment shows the worst performance, aligning only 37.3% of the time\n",
    "   - This suggests sentiment for TSLA may be systematically misleading\n",
    "\n",
    "**Methodological Consistency**\n",
    "- No sentiment method demonstrates clear superiority\n",
    "- TextBlob shows the most variable performance (best for GOOGL, worst for TSLA)\n",
    "- The inconsistency across stocks suggests sentiment alignment is stock-specific rather than method-driven\n",
    "\n",
    "#### Critical Implications\n",
    "\n",
    "The alignment results provide the final, compelling evidence against our hypothesis. Not only are correlations statistically insignificant, but sentiment fails to provide even basic directional accuracy above random chance. The fact that overall alignment underperforms random expectations suggests that using Reddit sentiment as a directional indicator would be counterproductive for investment decisions.\n",
    "\n",
    "The results align with our earlier findings: the relationship between Reddit sentiment and stock returns is essentially non-existent in our dataset, regardless of how we measure or analyze it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7be04-3c23-4475-bbcc-463410d8b0ad",
   "metadata": {},
   "source": [
    "# 8. Synthesis & Interpretation\n",
    "\n",
    "## 8.1 Hypothesis Resolution\n",
    "**The evidence strongly rejects our initial hypothesis.**  \n",
    "We hypothesized a \"statistically significant positive correlation between weekly Reddit sentiment scores and weekly stock returns,\" but comprehensive testing across three analytical approaches reveals:\n",
    "\n",
    "- **Zero statistically significant correlations** across 9 stock-method combinations\n",
    "- **Correlation coefficients indistinguishable from random noise** (average r = 0.052) with wide confidence intervals\n",
    "- **Alignment rates that underperform random chance expectations**, suggesting sentiment fails even as a basic directional indicator\n",
    "\n",
    "The consistency of null findings across all three sentiment measurement methods (VADER, TextBlob, Combined) and all three technology stocks provides compelling multi-faceted evidence against the hypothesized relationship.\n",
    "\n",
    "## 8.2 Key Findings Summary\n",
    "\n",
    "1. **No Statistical Significance**  \n",
    "All correlation p-values (0.386-0.889) far exceed the 0.05 significance threshold, with **all 95% confidence intervals including zero**\n",
    "\n",
    "2. **Negligible Practical Impact**  \n",
    "Even if correlations were significant, sentiment explains only **0.6% of return variation** on average (R² = 0.006), with the strongest relationship explaining just 1.5%\n",
    "\n",
    "3. **Directional Inaccuracy**  \n",
    "Sentiment alignment **underperforms random chance** by 1.7% overall, with only 3 of 9 method-stock combinations beating expected random alignment\n",
    "\n",
    "4. **Methodological Consistency**  \n",
    "All three sentiment analysis approaches (VADER, TextBlob, Combined) yield similarly weak results, with **identical confidence interval widths** (avg 0.548) across methods\n",
    "\n",
    "5. **Stock-Specific Patterns**  \n",
    "TSLA shows the weakest alignment (**underperforming random chance by 3.2-10.8%**), potentially reflecting its controversial nature among investors\n",
    "\n",
    "6. **Visual Confirmation**  \n",
    "Scatterplots show genuinely scattered data with **no discernible linear patterns** or influential outliers across all sentiment methods\n",
    "\n",
    "## 8.3 Limitations & Implications\n",
    "\n",
    "**Critical Limitations from Our Analysis**\n",
    "- **Overwhelming Sentiment Bias**  \n",
    "  With 76-98% of weeks showing positive sentiment, we essentially tested \"degrees of optimism\" rather than true sentiment direction\n",
    "- **Bull Market Context**  \n",
    "  The strong upward trend in all stocks during our study period created a challenging environment for detecting sentiment signals against the prevailing market momentum\n",
    "- **Temporal Clustering**  \n",
    "  The mid-period sentiment decline across all stocks suggests market-wide factors may have confounded stock-specific relationships\n",
    "- **Platform Specificity**  \n",
    "  Findings are limited to Reddit's user demographics and may not generalize to other social media platforms or investor populations\n",
    "\n",
    "**Practical Implications**\n",
    "- **For Investors**  \n",
    "  Reddit sentiment provides no reliable signal for short-term trading decisions and would likely underperform simple random guessing given the alignment results\n",
    "- **For Researchers**  \n",
    "  Weekly aggregation may be too coarse to capture the fast-moving nature of social media sentiment's impact on markets\n",
    "- **For Methodology**  \n",
    "  The consistency of null results across three sentiment measurement methods suggests the issue lies not in sentiment quantification but in the fundamental relationship itself\n",
    "\n",
    "## 8.4 Future Research Directions\n",
    "\n",
    "1. **Higher Frequency Analysis**  \n",
    "Investigate daily, hourly, or intraday relationships to capture faster sentiment-return dynamics that may be obscured by weekly aggregation, potentially revealing shorter-term relationships.\n",
    "\n",
    "2. **Sentiment Neutralization Methods**  \n",
    "Develop techniques to account for the overwhelming positivity bias in financial social media, enabling better detection of meaningful sentiment shifts amid generally optimistic discourse.\n",
    "\n",
    "3. **Event-Study Approach**  \n",
    "Focus analysis on specific market-moving events (earnings announcements, product launches, regulatory news) where sentiment may have clearer, less confounded relationships with price movements.\n",
    "\n",
    "4. **Multi-Platform Analysis**  \n",
    "Incorporate data from Twitter, StockTwits, and other financial platforms to create a more comprehensive sentiment measure and test generalizability beyond Reddit's user base.\n",
    "\n",
    "5. **Alternative Sentiment Methodologies**  \n",
    "Explore more sophisticated NLP approaches including aspect-based sentiment analysis to distinguish between discussions about different company facets (products, management, financials).\n",
    "\n",
    "6. **Control Variable Integration**  \n",
    "Incorporate known market factors (volatility indices, sector performance, macroeconomic data) to isolate sentiment's unique contribution from other market drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d9bf0-f5f4-48b9-a495-8171ee095ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
